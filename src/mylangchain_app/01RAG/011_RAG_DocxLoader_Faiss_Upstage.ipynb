{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poetry add docx2txt\n",
    "# poetry add langchain-upstage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Knowledge Base 구성을 위한 데이터 생성\n",
    "\n",
    "- [RecursiveCharacterTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/)를 활용한 데이터 chunking\n",
    "    - split 된 데이터 chunk를 Large Language Model(LLM)에게 전달하면 토큰 절약 가능\n",
    "    - 비용 감소와 답변 생성시간 감소의 효과\n",
    "    - LangChain에서 다양한 [TextSplitter](https://python.langchain.com/v0.2/docs/how_to/#text-splitters)들을 제공\n",
    "- `chunk_size` 는 split 된 chunk의 최대 크기\n",
    "- `chunk_overlap`은 앞 뒤로 나뉘어진 chunk들이 얼마나 겹쳐도 되는지 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "loader = Docx2txtLoader('../data/tax_with_table.docx')\n",
    "document_list = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "print(len(document_list))\n",
    "print(type(document_list[0]))\n",
    "print(document_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 데이터를 처음 저장할 때 \n",
    "database = FAISS.from_documents(documents=document_list, embedding=embeddings)\n",
    "# 로컬 파일로 저장\n",
    "database.save_local(\"./faiss_db\")\n",
    "\n",
    "# 이미 저장된 데이터를 사용할 때 \n",
    "# database = FAISS.load_local(\"faiss_docdb\", embedding, allow_dangerous_deserialization=True)\n",
    "print(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 답변 생성을 위한 Retrieval\n",
    "\n",
    "- `FAISS`에 저장한 데이터를 유사도 검색(`similarity_search()`)를 활용해서 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '과세소득의 범위 및 소득의 구분에는 어떤것들이 있나요?'\n",
    "\n",
    "# `k` 값을 조절해서 얼마나 많은 데이터를 불러올지 결정\n",
    "retrieved_docs = database.similarity_search(query, k=6)\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "print(type(retrieved_docs[0]))\n",
    "print(retrieved_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Augmentation을 위한 Prompt 활용\n",
    "\n",
    "- Retrieval된 데이터는 LangChain에서 제공하는 프롬프트(`\"rlm/rag-prompt\"`) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "print(\"===> 6. 생성 → LLM으로 답변 생성\")\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 답변 생성\n",
    "\n",
    "- [RetrievalQA](https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain)를 통해 LLM에 전달\n",
    "    - `RetrievalQA`는 [create_retrieval_chain](https://python.langchain.com/v0.2/docs/how_to/qa_sources/#using-create_retrieval_chain)으로 대체됨\n",
    "    - 실제 ChatBot 구현 시 `create_retrieval_chain`으로 변경하는 과정을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from pprint import pprint\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, \n",
    "    retriever=database.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '과세소득의 범위 및 소득의 구분에는 어떤것들이 있나요?'\n",
    "\n",
    "ai_message = qa_chain.invoke({\"query\": query})\n",
    "pprint(ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '비과세소득에 어떤 것들이 있나요?'\n",
    "\n",
    "ai_message = qa_chain.invoke({\"query\": query})\n",
    "pprint(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LangChain 기반의 RAG(Retrieval-Augmented Generation) 파이프라인을 구현하여 DOCX 문서를 로드, 임베딩, 검색, 그리고 LLM을 통한 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # 특정 경고 유형만 무시\n",
    "\n",
    "#  1. 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API 키 확인\n",
    "OPENAI_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"UPSTAGE API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "#  3. DOCX 파일 로드 및 텍스트 추출 (Docx2txtLoader 활용)\n",
    "def load_docx(file_path):\n",
    "    \"\"\"DOCX 파일에서 텍스트를 추출하는 함수.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "#  4. 문서 분할 함수\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"텍스트를 지정된 크기의 청크로 분할하는 함수.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "#  5. 벡터 데이터베이스(FAISS) 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"텍스트 청크를 임베딩하고 FAISS 벡터 저장소에 저장.\"\"\"\n",
    "    try:\n",
    "        documents = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        vector_store.save_local(\"faiss_docdb\")\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "#  6. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store):\n",
    "    \"\"\"LLM을 사용하여 검색된 문서 기반으로 답변 생성.\"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatUpstage(\n",
    "            model=\"solar-pro\",\n",
    "            base_url=\"https://api.upstage.ai/v1\",\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        # 프롬프트 로드 (RAG 최적화된 LangChain Hub 프롬프트 사용)\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "        # RetrievalQA 체인 생성\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm, \n",
    "            retriever=vector_store.as_retriever(),\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "\n",
    "        # LLM 응답 생성\n",
    "        ai_message = qa_chain.invoke({\"query\": query})\n",
    "        print(ai_message)\n",
    "        return ai_message[\"result\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "#  실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX 파일 경로\n",
    "    docx_path = \"../data/tax_with_table.docx\"\n",
    "    \n",
    "    # 1. 문서 로드\n",
    "    text = load_docx(docx_path)\n",
    "    print(\"문서 로드 완료\")\n",
    "    \n",
    "    # 2. 문서 분할\n",
    "    text_chunks = split_text(text)\n",
    "    print(f\"문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 3. 임베딩 모델 초기화\n",
    "    embedding_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    \n",
    "    # 4. 벡터 저장소 생성\n",
    "    vector_store = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5. 질의 실행\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store)\n",
    "    \n",
    "    # 6. AI 응답 출력\n",
    "    print(\"\\n AI의 답변:\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  개선된 Source Level1\n",
    "```python\n",
    "RuntimeError: 벡터 저장소 생성 실패: Error code: 400 - \n",
    "{'error': {\n",
    "    'message': 'Requested 313741 tokens, max 300000 tokens per request', \n",
    "    'type': 'max_tokens_per_request', \n",
    "    'param': None, 'code': 'max_tokens_per_request'\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "#  1. 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API 키 확인\n",
    "OPENAI_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"UPSTAGE API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "# 2. 한국어 법률 문서 전용 텍스트 전처리 함수\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"한국어 법률 문서를 위한 전처리.\"\"\"\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 조항 번호 정규화 (제1조, 제2조 등)\n",
    "    text = re.sub(r'제(\\d+)조', r'제\\1조', text)\n",
    "    \n",
    "    # 항 번호 정규화\n",
    "    text = re.sub(r'①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩', \n",
    "                  lambda m: f\"제{ord(m.group()) - ord('①') + 1}항\", text)\n",
    "    \n",
    "    # 호 번호 정규화\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'제\\1호 ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. 개선된 문서 분할 함수\n",
    "def advanced_split_text(text, chunk_size=800, chunk_overlap=200):\n",
    "    \"\"\"법률 문서에 최적화된 텍스트 분할.\"\"\"\n",
    "    # 전처리\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # 법률 문서 구조를 고려한 분할자\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n제\", \"\\n**제\",  # 조항 분할\n",
    "            \"\\n①\", \"\\n②\", \"\\n③\", \"\\n④\", \"\\n⑤\",  # 항 분할\n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # 호 분할\n",
    "            \"\\n가.\", \"\\n나.\", \"\\n다.\", \"\\n라.\", \"\\n마.\",  # 목 분할\n",
    "            \"\\n\\n\",  # 문단 분할\n",
    "            \"\\n\",    # 줄 분할\n",
    "            \". \",    # 문장 분할\n",
    "            \" \",     # 단어 분할\n",
    "            \"\"       # 문자 분할\n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. 개선된 문서 로더\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"개선된 DOCX 파일 로더.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다.\")\n",
    "        \n",
    "        # 기본 정리\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # 여러 개의 빈 줄을 두 개로 통일\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # 여러 공백을 하나로 통일\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "# 5. 벡터 저장소 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"메타데이터가 포함된 벡터 저장소 생성.\"\"\"\n",
    "    try:\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # 메타데이터 추가\n",
    "            metadata = {\n",
    "                'chunk_id': i,\n",
    "                'chunk_length': len(chunk),\n",
    "                'chunk_type': 'legal_document'\n",
    "            }\n",
    "            \n",
    "            # 조항 정보 추출\n",
    "            if '제' in chunk and '조' in chunk:\n",
    "                article_match = re.search(r'제(\\d+)조', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"제{article_match.group(1)}조\"\n",
    "            \n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        # \n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        vector_store.save_local(\"faiss_docdb\")\n",
    "        return vector_store, documents\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "# 6. 키워드 기반 검색 함수\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"간단한 키워드 기반 검색.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서의 점수 계산\n",
    "    scores = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 교집합 단어 수로 점수 계산\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # 정확한 구문 매칭 보너스\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # 점수순 정렬하여 상위 k개 반환\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. 하이브리드 검색 함수\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"벡터 검색과 키워드 검색을 결합한 하이브리드 검색.\"\"\"\n",
    "    \n",
    "    # 1. 벡터 유사도 검색\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)  # 더 많이 가져와서 다양성 확보\n",
    "    \n",
    "    # 2. 키워드 검색\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. 결과 합치기 및 점수 계산\n",
    "    combined_results = {}\n",
    "    \n",
    "    # 벡터 검색 결과 점수 (alpha 가중치)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        doc_id = doc.page_content\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1\n",
    "        }\n",
    "    \n",
    "    # 키워드 검색 결과 점수 ((1-alpha) 가중치)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # 이미 있는 문서면 점수 합산\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # 새로운 문서면 추가\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. 점수순으로 정렬하여 상위 k개 반환\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. 한국어 법률 문서 전용 프롬프트 생성 함수\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"한국어 법률 문서용 프롬프트 템플릿.\"\"\"\n",
    "    template = \"\"\"당신은 한국 세법 전문가입니다. 주어진 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해야 합니다.\n",
    "\n",
    "다음 규칙을 반드시 따르세요:\n",
    "1. 법조문의 조항, 항, 호, 목을 정확히 인용하세요\n",
    "2. 전문 용어를 사용할 때는 쉬운 설명을 함께 제공하세요\n",
    "3. 관련 조항들 간의 연관성을 설명하세요\n",
    "4. 실무적 적용 방법도 함께 설명하세요\n",
    "5. 불확실한 내용이 있으면 명시적으로 언급하세요\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "위 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해주세요. 관련 조항을 인용하며 설명해주세요.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. 질문 응답 함수\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"개선된 LLM 기반 질문 응답.\"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatUpstage(\n",
    "            model=\"solar-pro\",\n",
    "            base_url=\"https://api.upstage.ai/v1\",\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        print(f\"질의: {query}\")\n",
    "        print(\"하이브리드 검색 수행 중...\")\n",
    "        \n",
    "        # 하이브리드 검색으로 관련 문서 찾기\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"검색된 관련 문서: {len(relevant_docs)}개\")\n",
    "        \n",
    "        # 컨텍스트 구성\n",
    "        context = \"\\n\\n\".join([f\"[문서 {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # 한국어 법률 문서용 프롬프트\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # 프롬프트 생성\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM 응답 생성 중...\")\n",
    "        \n",
    "        # LLM 응답 생성\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "# 10. 컨텍스트 품질 평가 함수\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"검색된 문서의 품질을 간단히 평가.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 키워드 매칭 점수\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # 문서 길이 점수 (너무 짧거나 길지 않은 것이 좋음)\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # 종합 점수\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality\n",
    "\n",
    "# 실행 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX 파일 경로\n",
    "    docx_path = \"../data/tax_with_table.docx\"\n",
    "    \n",
    "    print(\"=== 개선된 RAG 파이프라인 실행 ===\\n\")\n",
    "    \n",
    "    # 1. 문서 로드\n",
    "    print(\"1. 문서 로드 중...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   문서 로드 완료: {len(text):,} 문자\\n\")\n",
    "    \n",
    "    # 2. 개선된 문서 분할\n",
    "    print(\"2. 문서 분할 중...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=800, chunk_overlap=200)\n",
    "    print(f\"   문서 분할 완료: {len(text_chunks)}개 청크 생성\\n\")\n",
    "    \n",
    "    # 3. 임베딩 모델 초기화\n",
    "    print(\"3. 임베딩 모델 초기화...\")\n",
    "    embedding_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    print(\"   임베딩 모델 초기화 완료\\n\")\n",
    "    \n",
    "    # 4. 벡터 저장소 생성\n",
    "    print(\"4. 벡터 저장소 생성 중...\")\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"    벡터 저장소 생성 완료\\n\")\n",
    "    \n",
    "    # 5. 질의 실행\n",
    "    print(\"5. 질의 실행 중...\")\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# 6. 컨텍스트 품질 평가\n",
    "context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "\n",
    "# 7. 결과 출력\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" AI의 답변:\")\n",
    "print(\"=\"*60)\n",
    "print(results[\"answer\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 검색 결과 요약:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"• 참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "print(f\"• 컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "print(f\"• 총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📄 참고한 문서 미리보기:\")\n",
    "print(\"=\"*60)\n",
    "for i, doc in enumerate(results[\"source_documents\"][:3]):  # 상위 3개만 미리보기\n",
    "    preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "    print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  개선된 Source Level2\n",
    "* Prompt 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. 환경 변수 로드 및 API 키 설정\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API 키 확인\n",
    "OPENAI_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"UPSTAGE API 키가 설정되지 않았습니다. .env 파일을 확인하세요.\")\n",
    "\n",
    "# 2. 한국어 법률 문서 전용 텍스트 전처리 함수\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"\n",
    "    한국어 법률 문서의 구조를 고려한 텍스트 전처리 함수\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 전처리된 텍스트\n",
    "        \n",
    "    주요 처리 내용:\n",
    "        - 불필요한 공백 및 개행 정리\n",
    "        - 법조문 번호 정규화 (제1조, 제2조 등)\n",
    "        - 항 번호를 아라비아 숫자로 변환 (①→제1항)\n",
    "        - 호 번호 정규화\n",
    "    \"\"\"\n",
    "    # 연속된 공백을 하나의 공백으로 통일\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 조항 번호 정규화: \"제 1 조\" -> \"제1조\" 형태로 통일\n",
    "    text = re.sub(r'제(\\d+)조', r'제\\1조', text)\n",
    "    \n",
    "    # 원문자 항 번호를 아라비아 숫자로 변환하여 검색 정확도 향상\n",
    "    # ①②③④⑤⑥⑦⑧⑨⑩ -> 제1항, 제2항, ... 제10항\n",
    "    text = re.sub(r'①|②|③|④|⑤|⑥|⑦|⑧|⑨|⑩', \n",
    "                  lambda m: f\"제{ord(m.group()) - ord('①') + 1}항\", text)\n",
    "    \n",
    "    # 호 번호 정규화: \"1. \" -> \"제1호 \" 형태로 변환\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'제\\1호 ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. 법률 문서에 최적화된 텍스트 분할 함수\n",
    "def advanced_split_text(text, chunk_size=600, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    법률 문서의 구조적 특성을 고려한 지능적 텍스트 분할\n",
    "    \n",
    "    Args:\n",
    "        text (str): 분할할 텍스트\n",
    "        chunk_size (int): 각 청크의 목표 크기 (문자 수)\n",
    "        chunk_overlap (int): 청크 간 중복되는 문자 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 분할된 텍스트 청크들의 리스트\n",
    "        \n",
    "    특징:\n",
    "        - 법률 문서의 계층 구조(조>항>호>목)를 고려한 분할 우선순위\n",
    "        - 의미적 완성도를 유지하면서 분할\n",
    "        - 토큰 한도를 고려한 적절한 크기 설정\n",
    "    \"\"\"\n",
    "    # 텍스트 전처리 수행\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # 법률 문서 구조를 고려한 분할 구분자들을 우선순위대로 설정\n",
    "    # 상위 구조부터 하위 구조 순서로 분할을 시도\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\n제\", \"\\n**제\",  # 조항 단위 분할 (가장 우선)\n",
    "            \"\\n①\", \"\\n②\", \"\\n③\", \"\\n④\", \"\\n⑤\",  # 항 단위 분할\n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # 호 단위 분할\n",
    "            \"\\n가.\", \"\\n나.\", \"\\n다.\", \"\\n라.\", \"\\n마.\",  # 목 단위 분할\n",
    "            \"\\n\\n\",  # 문단 단위 분할\n",
    "            \"\\n\",    # 줄 단위 분할\n",
    "            \". \",    # 문장 단위 분할\n",
    "            \" \",     # 단어 단위 분할\n",
    "            \"\"       # 문자 단위 분할 (최후 수단)\n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. DOCX 파일 로딩 및 전처리 함수\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"\n",
    "    DOCX 파일을 로드하고 기본적인 텍스트 정리를 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): DOCX 파일 경로\n",
    "        \n",
    "    Returns:\n",
    "        str: 정리된 텍스트\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: 파일 로딩 실패 시\n",
    "        ValueError: 텍스트 추출 실패 시\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Docx2txtLoader를 사용하여 DOCX 파일에서 텍스트 추출\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        # 텍스트가 비어있는지 확인\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"문서에서 텍스트를 추출할 수 없습니다. 파일이 비어있거나 손상되었을 수 있습니다.\")\n",
    "        \n",
    "        # 기본적인 텍스트 정리 작업\n",
    "        # 연속된 빈 줄을 두 개의 줄바꿈으로 통일\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # 연속된 공백과 탭을 하나의 공백으로 통일\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"문서 로딩 실패: {str(e)}\")\n",
    "\n",
    "# 5. 배치 처리를 통한 벡터 저장소 생성 함수\n",
    "def create_vector_store(text_chunks, embedding_model, batch_size=30):\n",
    "    \"\"\"\n",
    "    텍스트 청크들을 배치 단위로 처리하여 벡터 저장소 생성\n",
    "    토큰 한도 초과 문제를 해결하기 위해 배치 처리 방식 적용\n",
    "    \n",
    "    Args:\n",
    "        text_chunks (list): 분할된 텍스트 청크들\n",
    "        embedding_model: OpenAI 임베딩 모델 객체\n",
    "        batch_size (int): 한 번에 처리할 청크 수 (기본값: 30)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (FAISS 벡터 저장소, Document 객체들의 리스트)\n",
    "        \n",
    "    처리 과정:\n",
    "        1. 각 청크에 메타데이터 추가 (ID, 길이, 조항 정보 등)\n",
    "        2. 청크 크기가 너무 큰 경우 자동으로 제한\n",
    "        3. 배치 단위로 임베딩 생성하여 토큰 한도 문제 방지\n",
    "        4. FAISS merge 기능을 활용하여 배치별 결과 통합\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"   총 {len(text_chunks)}개 청크를 {batch_size}개씩 배치 처리...\")\n",
    "        \n",
    "        # Document 객체들을 저장할 리스트 초기화\n",
    "        documents = []\n",
    "        \n",
    "        # 각 텍스트 청크를 Document 객체로 변환하면서 메타데이터 추가\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # 청크 크기가 너무 큰 경우 제한 (토큰 한도 방지)\n",
    "            if len(chunk) > 2000:\n",
    "                chunk = chunk[:2000] + \"...\"\n",
    "                print(f\"   경고: 청크 {i}가 너무 커서 2000자로 제한했습니다.\")\n",
    "            \n",
    "            # 각 청크에 추가할 메타데이터 구성\n",
    "            metadata = {\n",
    "                'chunk_id': i,  # 청크 고유 번호\n",
    "                'chunk_length': len(chunk),  # 청크 길이\n",
    "                'chunk_type': 'legal_document'  # 문서 유형\n",
    "            }\n",
    "            \n",
    "            # 청크 내용에서 조항 정보 자동 추출\n",
    "            # \"제n조\" 패턴을 찾아 메타데이터에 추가\n",
    "            if '제' in chunk and '조' in chunk:\n",
    "                article_match = re.search(r'제(\\d+)조', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"제{article_match.group(1)}조\"\n",
    "            \n",
    "            # Document 객체 생성하여 리스트에 추가\n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        \n",
    "        # 배치별 벡터 저장소 생성 및 병합 과정\n",
    "        vector_store = None\n",
    "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # 문서들을 배치 크기만큼 나누어 처리\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            \n",
    "            print(f\"   배치 {batch_num}/{total_batches} 처리 중... ({len(batch_docs)}개 문서)\")\n",
    "            \n",
    "            # 첫 번째 배치인 경우 새로운 벡터 저장소 생성\n",
    "            if vector_store is None:\n",
    "                vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "            else:\n",
    "                # 이후 배치들은 기존 벡터 저장소에 병합\n",
    "                batch_vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "                vector_store.merge_from(batch_vector_store)\n",
    "        \n",
    "        print(\"   모든 배치 처리 완료\")\n",
    "        return vector_store, documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"벡터 저장소 생성 실패: {str(e)}\")\n",
    "\n",
    "# 6. 키워드 기반 검색 함수\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"\n",
    "    단순한 키워드 매칭을 통한 문서 검색\n",
    "    벡터 검색과 상호 보완적으로 사용하여 검색 정확도 향상\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 반환할 상위 문서 수\n",
    "        \n",
    "    Returns:\n",
    "        list: 관련도 순으로 정렬된 Document 객체들\n",
    "        \n",
    "    검색 로직:\n",
    "        1. 질의와 문서의 단어 교집합 계산\n",
    "        2. 교집합 크기를 질의 단어 수로 나누어 정규화\n",
    "        3. 정확한 구문 매칭 시 보너스 점수 부여\n",
    "        4. 점수 기준으로 상위 k개 문서 반환\n",
    "    \"\"\"\n",
    "    # 질의를 소문자로 변환하고 단어 단위로 분할\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서의 점수를 계산할 리스트\n",
    "    scores = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # 문서 내용을 소문자로 변환하고 단어 단위로 분할\n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 질의 단어와 문서 단어의 교집합 계산\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        \n",
    "        # 기본 점수: 교집합 크기를 질의 단어 수로 나누어 정규화 (0~1 범위)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # 보너스 점수: 질의 전체가 문서에 정확히 포함된 경우\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        # (점수, 인덱스, 문서) 튜플로 저장\n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # 점수 기준으로 내림차순 정렬하여 상위 k개 반환\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. 하이브리드 검색 함수 (벡터 + 키워드 검색 결합)\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"\n",
    "    벡터 유사도 검색과 키워드 검색을 결합한 하이브리드 검색\n",
    "    두 검색 방법의 장점을 결합하여 더 정확한 검색 결과 제공\n",
    "    \n",
    "    Args:\n",
    "        query (str): 검색 질의\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        k (int): 최종 반환할 문서 수\n",
    "        alpha (float): 벡터 검색 가중치 (0~1, 높을수록 벡터 검색 중시)\n",
    "        \n",
    "    Returns:\n",
    "        list: 종합 점수로 정렬된 상위 k개 Document 객체들\n",
    "        \n",
    "    검색 과정:\n",
    "        1. 벡터 유사도 검색으로 의미적으로 관련된 문서 찾기\n",
    "        2. 키워드 검색으로 정확한 용어 매칭 문서 찾기\n",
    "        3. 두 결과를 alpha 가중치로 결합\n",
    "        4. 중복 문서 처리 및 최종 점수 계산\n",
    "        5. 점수 순으로 정렬하여 상위 k개 반환\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 벡터 유사도 검색 수행\n",
    "    # 더 많은 후보를 가져와서 다양성 확보 (k*2개)\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)\n",
    "    \n",
    "    # 2. 키워드 기반 검색 수행\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. 두 검색 결과를 점수와 함께 통합\n",
    "    combined_results = {}\n",
    "    \n",
    "    # 벡터 검색 결과에 점수 부여 (alpha 가중치 적용)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        # 문서 내용을 고유 키로 사용\n",
    "        doc_id = doc.page_content\n",
    "        # 순위가 높을수록 높은 점수 (1.0에서 시작하여 순위에 따라 감소)\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        \n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1,\n",
    "            'keyword_rank': None\n",
    "        }\n",
    "    \n",
    "    # 키워드 검색 결과에 점수 부여 ((1-alpha) 가중치 적용)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # 이미 벡터 검색에서 찾은 문서인 경우 점수 합산\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # 키워드 검색에서만 찾은 새로운 문서인 경우 추가\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'vector_rank': None,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. 종합 점수 기준으로 정렬하여 상위 k개 반환\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. 한국어 법률 문서 전용 프롬프트 생성 함수\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"\n",
    "    한국어 법률 문서 특성에 맞춘 전용 프롬프트 템플릿 생성\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: 법률 문서 질의응답을 위한 프롬프트 템플릿\n",
    "        \n",
    "    프롬프트 특징:\n",
    "        - 법조문 인용의 정확성 강조\n",
    "        - 전문 용어에 대한 쉬운 설명 요구\n",
    "        - 조항 간 연관성 설명 포함\n",
    "        - 실무적 적용 방법 제시\n",
    "        - 불확실한 내용에 대한 명시적 언급\n",
    "    \"\"\"\n",
    "    template = \"\"\"당신은 한국 세법 전문가입니다. 주어진 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해야 합니다.\n",
    "\n",
    "다음 규칙을 반드시 따르세요:\n",
    "1. 법조문의 조항, 항, 호, 목을 정확히 인용하세요\n",
    "2. 전문 용어를 사용할 때는 쉬운 설명을 함께 제공하세요\n",
    "3. 관련 조항들 간의 연관성을 설명하세요\n",
    "4. 실무적 적용 방법도 함께 설명하세요\n",
    "5. 불확실한 내용이 있으면 명시적으로 언급하세요\n",
    "\n",
    "참고 문서:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "위 법률 문서를 바탕으로 정확하고 자세한 답변을 제공해주세요. 관련 조항을 인용하며 설명해주세요.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. LLM을 활용한 질문 응답 함수\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"\n",
    "    하이브리드 검색과 고성능 LLM을 결합한 질문 응답 시스템\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        documents (list): Document 객체들의 리스트\n",
    "        \n",
    "    Returns:\n",
    "        dict: 답변, 참고 문서, 사용된 컨텍스트를 포함한 응답 딕셔너리\n",
    "        \n",
    "    처리 과정:\n",
    "        1. GPT-4o-mini 모델로 LLM 초기화 (높은 정확도)\n",
    "        2. 하이브리드 검색으로 관련 문서 7개 검색\n",
    "        3. 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        4. 법률 문서 전용 프롬프트 적용\n",
    "        5. LLM으로 최종 답변 생성\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # LLM 모델 설정\n",
    "        llm = ChatUpstage(\n",
    "            model=\"solar-pro\",\n",
    "            base_url=\"https://api.upstage.ai/v1\",\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        print(f\"질의: {query}\")\n",
    "        print(\"하이브리드 검색 수행 중...\")\n",
    "        \n",
    "        # 하이브리드 검색으로 관련 문서 검색\n",
    "        # k=7로 설정하여 충분한 컨텍스트 확보\n",
    "        # alpha=0.7로 설정하여 벡터 검색을 더 중시 (의미적 유사도 우선)\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"검색된 관련 문서: {len(relevant_docs)}개\")\n",
    "        \n",
    "        # 검색된 문서들을 하나의 컨텍스트로 결합\n",
    "        # 각 문서에 번호를 매겨 구분하기 쉽게 구성\n",
    "        context = \"\\n\\n\".join([f\"[문서 {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # 한국어 법률 문서에 특화된 프롬프트 사용\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # 최종 프롬프트 생성 (컨텍스트와 질문 삽입)\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM 응답 생성 중...\")\n",
    "        \n",
    "        # LLM에 프롬프트 전달하여 답변 생성\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # 결과를 딕셔너리 형태로 반환\n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM 응답 생성 실패: {str(e)}\")\n",
    "\n",
    "# 10. 검색 품질 평가 함수\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    검색된 문서들의 품질을 정량적으로 평가\n",
    "    \n",
    "    Args:\n",
    "        query (str): 원본 질의\n",
    "        retrieved_docs (list): 검색된 Document 객체들\n",
    "        \n",
    "    Returns:\n",
    "        float: 0~1 범위의 품질 점수 (1에 가까울수록 높은 품질)\n",
    "        \n",
    "    평가 기준:\n",
    "        1. 키워드 매칭률 (70% 가중치): 질의 단어가 문서에 포함된 비율\n",
    "        2. 문서 길이 적절성 (30% 가중치): 너무 짧거나 길지 않은 적절한 길이\n",
    "    \"\"\"\n",
    "    # 질의를 단어 단위로 분할하고 소문자 변환\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # 각 문서별 품질 점수 계산\n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        # 문서 내용을 단어 단위로 분할하고 소문자 변환\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # 키워드 매칭 점수 계산\n",
    "        # 질의 단어 중 문서에 포함된 단어의 비율\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # 문서 길이 점수 계산\n",
    "        # 1000자를 기준으로 정규화 (1000자 이상이면 1.0점)\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # 종합 점수 계산 (키워드 매칭 70% + 길이 적절성 30%)\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    # 전체 문서의 평균 품질 점수 반환\n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # 처리할 DOCX 파일 경로 설정\n",
    "    docx_path = \"data/tax_with_table.docx\"\n",
    "    \n",
    "    print(\"개선된 RAG 파이프라인 실행\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1단계: 문서 로드\n",
    "    print(\"1. 문서 로드 중...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   문서 로드 완료: {len(text):,} 문자\")\n",
    "    \n",
    "    # 2단계: 문서 분할\n",
    "    print(\"\\n2. 문서 분할 중...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=600, chunk_overlap=100)\n",
    "    print(f\"   문서 분할 완료: {len(text_chunks)}개 청크 생성\")\n",
    "    \n",
    "    # 청크 크기 통계 분석 및 출력\n",
    "    chunk_lengths = [len(chunk) for chunk in text_chunks]\n",
    "    avg_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "    max_length = max(chunk_lengths)\n",
    "    print(f\"   평균 청크 길이: {avg_length:.0f}자, 최대 길이: {max_length}자\")\n",
    "    \n",
    "    # 3단계: 임베딩 모델 초기화\n",
    "    print(\"\\n3. 임베딩 모델 초기화...\")\n",
    "    \n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "    )\n",
    "    print(\"   임베딩 모델 초기화 완료\")\n",
    "    \n",
    "    # 4단계: 벡터 저장소 생성\n",
    "    print(\"\\n4. 벡터 저장소 생성 중...\")\n",
    "    # 배치 크기 30으로 설정하여 토큰 한도 문제 방지\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model, batch_size=30)\n",
    "    print(\"   벡터 저장소 생성 완료\")\n",
    "    \n",
    "    # 5단계: 질의 실행\n",
    "    print(\"\\n5. 질의 실행 중...\")\n",
    "   #query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    query = \"비과세소득의 종류에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6단계: 검색 품질 평가\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7단계: 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AI의 답변:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"검색 결과 요약:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "    print(f\"컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "    print(f\"총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"참고한 문서 미리보기:\")\n",
    "    print(\"=\" * 60)\n",
    "    # 상위 3개 문서의 일부만 미리보기로 출력\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200자까지만 미리보기로 표시\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "            # 5단계: 질의 실행\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n5. 질의 실행 중...\")\n",
    "    query = \"총수입금액 불산입에 대하여 설명해 주세요.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6단계: 검색 품질 평가\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7단계: 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AI의 답변:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"검색 결과 요약:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"참고한 문서 조각 수: {len(results['source_documents'])}개\")\n",
    "    print(f\"컨텍스트 품질 점수: {context_quality:.2f}/1.00\")\n",
    "    print(f\"총 컨텍스트 길이: {len(results['context_used']):,} 문자\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"참고한 문서 미리보기:\")\n",
    "    print(\"=\" * 60)\n",
    "    # 상위 3개 문서의 일부만 미리보기로 출력\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200자까지만 미리보기로 표시\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[문서 {i+1}] {preview}\")\n",
    "    print(\"=\" * 60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
