{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poetry add docx2txt\n",
    "# poetry add langchain-upstage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Knowledge Base êµ¬ì„±ì„ ìœ„í•œ ë°ì´í„° ìƒì„±\n",
    "\n",
    "- [RecursiveCharacterTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/)ë¥¼ í™œìš©í•œ ë°ì´í„° chunking\n",
    "    - split ëœ ë°ì´í„° chunkë¥¼ Large Language Model(LLM)ì—ê²Œ ì „ë‹¬í•˜ë©´ í† í° ì ˆì•½ ê°€ëŠ¥\n",
    "    - ë¹„ìš© ê°ì†Œì™€ ë‹µë³€ ìƒì„±ì‹œê°„ ê°ì†Œì˜ íš¨ê³¼\n",
    "    - LangChainì—ì„œ ë‹¤ì–‘í•œ [TextSplitter](https://python.langchain.com/v0.2/docs/how_to/#text-splitters)ë“¤ì„ ì œê³µ\n",
    "- `chunk_size` ëŠ” split ëœ chunkì˜ ìµœëŒ€ í¬ê¸°\n",
    "- `chunk_overlap`ì€ ì• ë’¤ë¡œ ë‚˜ë‰˜ì–´ì§„ chunkë“¤ì´ ì–¼ë§ˆë‚˜ ê²¹ì³ë„ ë˜ëŠ”ì§€ ì§€ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "loader = Docx2txtLoader('../data/tax_with_table.docx')\n",
    "document_list = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "print(len(document_list))\n",
    "print(type(document_list[0]))\n",
    "print(document_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# ë°ì´í„°ë¥¼ ì²˜ìŒ ì €ì¥í•  ë•Œ \n",
    "database = FAISS.from_documents(documents=document_list, embedding=embeddings)\n",
    "# ë¡œì»¬ íŒŒì¼ë¡œ ì €ì¥\n",
    "database.save_local(\"./faiss_db\")\n",
    "\n",
    "# ì´ë¯¸ ì €ì¥ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ë•Œ \n",
    "# database = FAISS.load_local(\"faiss_docdb\", embedding, allow_dangerous_deserialization=True)\n",
    "print(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ë‹µë³€ ìƒì„±ì„ ìœ„í•œ Retrieval\n",
    "\n",
    "- `FAISS`ì— ì €ì¥í•œ ë°ì´í„°ë¥¼ ìœ ì‚¬ë„ ê²€ìƒ‰(`similarity_search()`)ë¥¼ í™œìš©í•´ì„œ ê°€ì ¸ì˜´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'ê³¼ì„¸ì†Œë“ì˜ ë²”ìœ„ ë° ì†Œë“ì˜ êµ¬ë¶„ì—ëŠ” ì–´ë–¤ê²ƒë“¤ì´ ìˆë‚˜ìš”?'\n",
    "\n",
    "# `k` ê°’ì„ ì¡°ì ˆí•´ì„œ ì–¼ë§ˆë‚˜ ë§ì€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ì§€ ê²°ì •\n",
    "retrieved_docs = database.similarity_search(query, k=6)\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "print(type(retrieved_docs[0]))\n",
    "print(retrieved_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Augmentationì„ ìœ„í•œ Prompt í™œìš©\n",
    "\n",
    "- Retrievalëœ ë°ì´í„°ëŠ” LangChainì—ì„œ ì œê³µí•˜ëŠ” í”„ë¡¬í”„íŠ¸(`\"rlm/rag-prompt\"`) ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "print(\"===> 6. ìƒì„± â†’ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±\")\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ë‹µë³€ ìƒì„±\n",
    "\n",
    "- [RetrievalQA](https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain)ë¥¼ í†µí•´ LLMì— ì „ë‹¬\n",
    "    - `RetrievalQA`ëŠ” [create_retrieval_chain](https://python.langchain.com/v0.2/docs/how_to/qa_sources/#using-create_retrieval_chain)ìœ¼ë¡œ ëŒ€ì²´ë¨\n",
    "    - ì‹¤ì œ ChatBot êµ¬í˜„ ì‹œ `create_retrieval_chain`ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ê³¼ì •ì„ ë³¼ ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from pprint import pprint\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, \n",
    "    retriever=database.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'ê³¼ì„¸ì†Œë“ì˜ ë²”ìœ„ ë° ì†Œë“ì˜ êµ¬ë¶„ì—ëŠ” ì–´ë–¤ê²ƒë“¤ì´ ìˆë‚˜ìš”?'\n",
    "\n",
    "ai_message = qa_chain.invoke({\"query\": query})\n",
    "pprint(ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'ë¹„ê³¼ì„¸ì†Œë“ì— ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?'\n",
    "\n",
    "ai_message = qa_chain.invoke({\"query\": query})\n",
    "pprint(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LangChain ê¸°ë°˜ì˜ RAG(Retrieval-Augmented Generation) íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ì—¬ DOCX ë¬¸ì„œë¥¼ ë¡œë“œ, ì„ë² ë”©, ê²€ìƒ‰, ê·¸ë¦¬ê³  LLMì„ í†µí•œ ë‹µë³€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # íŠ¹ì • ê²½ê³  ìœ í˜•ë§Œ ë¬´ì‹œ\n",
    "\n",
    "#  1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API í‚¤ í™•ì¸\n",
    "OPENAI_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"UPSTAGE API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "#  3. DOCX íŒŒì¼ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ (Docx2txtLoader í™œìš©)\n",
    "def load_docx(file_path):\n",
    "    \"\"\"DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "#  4. ë¬¸ì„œ ë¶„í•  í•¨ìˆ˜\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "#  5. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤(FAISS) ìƒì„± í•¨ìˆ˜\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  FAISS ë²¡í„° ì €ì¥ì†Œì— ì €ì¥.\"\"\"\n",
    "    try:\n",
    "        documents = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        vector_store.save_local(\"faiss_docdb\")\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "#  6. LLMì„ í™œìš©í•œ ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜\n",
    "def query_with_llm(query, vector_store):\n",
    "    \"\"\"LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±.\"\"\"\n",
    "    try:\n",
    "        # LLM ëª¨ë¸ ì„¤ì •\n",
    "        llm = ChatUpstage(\n",
    "            model=\"solar-pro\",\n",
    "            base_url=\"https://api.upstage.ai/v1\",\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ (RAG ìµœì í™”ëœ LangChain Hub í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)\n",
    "        prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "        # RetrievalQA ì²´ì¸ ìƒì„±\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm, \n",
    "            retriever=vector_store.as_retriever(),\n",
    "            chain_type_kwargs={\"prompt\": prompt}\n",
    "        )\n",
    "\n",
    "        # LLM ì‘ë‹µ ìƒì„±\n",
    "        ai_message = qa_chain.invoke({\"query\": query})\n",
    "        print(ai_message)\n",
    "        return ai_message[\"result\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "#  ì‹¤í–‰ ì˜ˆì œ\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX íŒŒì¼ ê²½ë¡œ\n",
    "    docx_path = \"../data/tax_with_table.docx\"\n",
    "    \n",
    "    # 1. ë¬¸ì„œ ë¡œë“œ\n",
    "    text = load_docx(docx_path)\n",
    "    print(\"ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    # 2. ë¬¸ì„œ ë¶„í• \n",
    "    text_chunks = split_text(text)\n",
    "    print(f\"ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(text_chunks)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "    \n",
    "    # 3. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    embedding_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    \n",
    "    # 4. ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "    vector_store = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    # 5. ì§ˆì˜ ì‹¤í–‰\n",
    "    query = \"ì´ìˆ˜ì…ê¸ˆì•¡ ë¶ˆì‚°ì…ì— ëŒ€í•˜ì—¬ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"\n",
    "    results = query_with_llm(query, vector_store)\n",
    "    \n",
    "    # 6. AI ì‘ë‹µ ì¶œë ¥\n",
    "    print(\"\\n AIì˜ ë‹µë³€:\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ê°œì„ ëœ Source Level1\n",
    "```python\n",
    "RuntimeError: ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: Error code: 400 - \n",
    "{'error': {\n",
    "    'message': 'Requested 313741 tokens, max 300000 tokens per request', \n",
    "    'type': 'max_tokens_per_request', \n",
    "    'param': None, 'code': 'max_tokens_per_request'\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "#  1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API í‚¤ í™•ì¸\n",
    "OPENAI_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"UPSTAGE API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 2. í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œ ì „ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œë¥¼ ìœ„í•œ ì „ì²˜ë¦¬.\"\"\"\n",
    "    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # ì¡°í•­ ë²ˆí˜¸ ì •ê·œí™” (ì œ1ì¡°, ì œ2ì¡° ë“±)\n",
    "    text = re.sub(r'ì œ(\\d+)ì¡°', r'ì œ\\1ì¡°', text)\n",
    "    \n",
    "    # í•­ ë²ˆí˜¸ ì •ê·œí™”\n",
    "    text = re.sub(r'â‘ |â‘¡|â‘¢|â‘£|â‘¤|â‘¥|â‘¦|â‘§|â‘¨|â‘©', \n",
    "                  lambda m: f\"ì œ{ord(m.group()) - ord('â‘ ') + 1}í•­\", text)\n",
    "    \n",
    "    # í˜¸ ë²ˆí˜¸ ì •ê·œí™”\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'ì œ\\1í˜¸ ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. ê°œì„ ëœ ë¬¸ì„œ ë¶„í•  í•¨ìˆ˜\n",
    "def advanced_split_text(text, chunk_size=800, chunk_overlap=200):\n",
    "    \"\"\"ë²•ë¥  ë¬¸ì„œì— ìµœì í™”ëœ í…ìŠ¤íŠ¸ ë¶„í• .\"\"\"\n",
    "    # ì „ì²˜ë¦¬\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # ë²•ë¥  ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë¶„í• ì\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\nì œ\", \"\\n**ì œ\",  # ì¡°í•­ ë¶„í• \n",
    "            \"\\nâ‘ \", \"\\nâ‘¡\", \"\\nâ‘¢\", \"\\nâ‘£\", \"\\nâ‘¤\",  # í•­ ë¶„í• \n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # í˜¸ ë¶„í• \n",
    "            \"\\nê°€.\", \"\\në‚˜.\", \"\\në‹¤.\", \"\\në¼.\", \"\\në§ˆ.\",  # ëª© ë¶„í• \n",
    "            \"\\n\\n\",  # ë¬¸ë‹¨ ë¶„í• \n",
    "            \"\\n\",    # ì¤„ ë¶„í• \n",
    "            \". \",    # ë¬¸ì¥ ë¶„í• \n",
    "            \" \",     # ë‹¨ì–´ ë¶„í• \n",
    "            \"\"       # ë¬¸ì ë¶„í• \n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. ê°œì„ ëœ ë¬¸ì„œ ë¡œë”\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"ê°œì„ ëœ DOCX íŒŒì¼ ë¡œë”.\"\"\"\n",
    "    try:\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        if not text.strip():\n",
    "            raise ValueError(\"ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ê¸°ë³¸ ì •ë¦¬\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # ì—¬ëŸ¬ ê°œì˜ ë¹ˆ ì¤„ì„ ë‘ ê°œë¡œ í†µì¼\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)  # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µì¼\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# 5. ë²¡í„° ì €ì¥ì†Œ ìƒì„± í•¨ìˆ˜\n",
    "def create_vector_store(text_chunks, embedding_model):\n",
    "    \"\"\"ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ë²¡í„° ì €ì¥ì†Œ ìƒì„±.\"\"\"\n",
    "    try:\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "            metadata = {\n",
    "                'chunk_id': i,\n",
    "                'chunk_length': len(chunk),\n",
    "                'chunk_type': 'legal_document'\n",
    "            }\n",
    "            \n",
    "            # ì¡°í•­ ì •ë³´ ì¶”ì¶œ\n",
    "            if 'ì œ' in chunk and 'ì¡°' in chunk:\n",
    "                article_match = re.search(r'ì œ(\\d+)ì¡°', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"ì œ{article_match.group(1)}ì¡°\"\n",
    "            \n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        # \n",
    "        vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "        vector_store.save_local(\"faiss_docdb\")\n",
    "        return vector_store, documents\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# 6. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # ê° ë¬¸ì„œì˜ ì ìˆ˜ ê³„ì‚°\n",
    "    scores = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # êµì§‘í•© ë‹¨ì–´ ìˆ˜ë¡œ ì ìˆ˜ ê³„ì‚°\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # ì •í™•í•œ êµ¬ë¬¸ ë§¤ì¹­ ë³´ë„ˆìŠ¤\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # ì ìˆ˜ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰.\"\"\"\n",
    "    \n",
    "    # 1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ ë‹¤ì–‘ì„± í™•ë³´\n",
    "    \n",
    "    # 2. í‚¤ì›Œë“œ ê²€ìƒ‰\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. ê²°ê³¼ í•©ì¹˜ê¸° ë° ì ìˆ˜ ê³„ì‚°\n",
    "    combined_results = {}\n",
    "    \n",
    "    # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì ìˆ˜ (alpha ê°€ì¤‘ì¹˜)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        doc_id = doc.page_content\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1\n",
    "        }\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì ìˆ˜ ((1-alpha) ê°€ì¤‘ì¹˜)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # ì´ë¯¸ ìˆëŠ” ë¬¸ì„œë©´ ì ìˆ˜ í•©ì‚°\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # ìƒˆë¡œìš´ ë¬¸ì„œë©´ ì¶”ê°€\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿.\"\"\"\n",
    "    template = \"\"\"ë‹¹ì‹ ì€ í•œêµ­ ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë²•ë¥  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:\n",
    "1. ë²•ì¡°ë¬¸ì˜ ì¡°í•­, í•­, í˜¸, ëª©ì„ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”\n",
    "2. ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ì‰¬ìš´ ì„¤ëª…ì„ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”\n",
    "3. ê´€ë ¨ ì¡°í•­ë“¤ ê°„ì˜ ì—°ê´€ì„±ì„ ì„¤ëª…í•˜ì„¸ìš”\n",
    "4. ì‹¤ë¬´ì  ì ìš© ë°©ë²•ë„ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”\n",
    "5. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì„¸ìš”\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ìœ„ ë²•ë¥  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. ê´€ë ¨ ì¡°í•­ì„ ì¸ìš©í•˜ë©° ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"ê°œì„ ëœ LLM ê¸°ë°˜ ì§ˆë¬¸ ì‘ë‹µ.\"\"\"\n",
    "    try:\n",
    "        # LLM ëª¨ë¸ ì„¤ì •\n",
    "        llm = ChatUpstage(\n",
    "            model=\"solar-pro\",\n",
    "            base_url=\"https://api.upstage.ai/v1\",\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        print(f\"ì§ˆì˜: {query}\")\n",
    "        print(\"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...\")\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ: {len(relevant_docs)}ê°œ\")\n",
    "        \n",
    "        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "        context = \"\\n\\n\".join([f\"[ë¬¸ì„œ {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œìš© í”„ë¡¬í”„íŠ¸\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # LLM ì‘ë‹µ ìƒì„±\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# 10. ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í’ˆì§ˆì„ ê°„ë‹¨íˆ í‰ê°€.\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # ë¬¸ì„œ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ ê²ƒì´ ì¢‹ìŒ)\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # ì¢…í•© ì ìˆ˜\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì œ\n",
    "if __name__ == \"__main__\":\n",
    "    # DOCX íŒŒì¼ ê²½ë¡œ\n",
    "    docx_path = \"../data/tax_with_table.docx\"\n",
    "    \n",
    "    print(\"=== ê°œì„ ëœ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ===\\n\")\n",
    "    \n",
    "    # 1. ë¬¸ì„œ ë¡œë“œ\n",
    "    print(\"1. ë¬¸ì„œ ë¡œë“œ ì¤‘...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(text):,} ë¬¸ì\\n\")\n",
    "    \n",
    "    # 2. ê°œì„ ëœ ë¬¸ì„œ ë¶„í• \n",
    "    print(\"2. ë¬¸ì„œ ë¶„í•  ì¤‘...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=800, chunk_overlap=200)\n",
    "    print(f\"   ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(text_chunks)}ê°œ ì²­í¬ ìƒì„±\\n\")\n",
    "    \n",
    "    # 3. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    print(\"3. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”...\")\n",
    "    embedding_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    print(\"   ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\\n\")\n",
    "    \n",
    "    # 4. ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "    print(\"4. ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...\")\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model)\n",
    "    print(\"    ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ\\n\")\n",
    "    \n",
    "    # 5. ì§ˆì˜ ì‹¤í–‰\n",
    "    print(\"5. ì§ˆì˜ ì‹¤í–‰ ì¤‘...\")\n",
    "    query = \"ì´ìˆ˜ì…ê¸ˆì•¡ ë¶ˆì‚°ì…ì— ëŒ€í•˜ì—¬ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# 6. ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€\n",
    "context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "\n",
    "# 7. ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" AIì˜ ë‹µë³€:\")\n",
    "print(\"=\"*60)\n",
    "print(results[\"answer\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"â€¢ ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(results['source_documents'])}ê°œ\")\n",
    "print(f\"â€¢ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜: {context_quality:.2f}/1.00\")\n",
    "print(f\"â€¢ ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(results['context_used']):,} ë¬¸ì\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“„ ì°¸ê³ í•œ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "print(\"=\"*60)\n",
    "for i, doc in enumerate(results[\"source_documents\"][:3]):  # ìƒìœ„ 3ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°\n",
    "    preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "    print(f\"\\n[ë¬¸ì„œ {i+1}] {preview}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ê°œì„ ëœ Source Level2\n",
    "* Prompt ê°œì„ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° API í‚¤ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "#  2. OpenAI API í‚¤ í™•ì¸\n",
    "OPENAI_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"UPSTAGE API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 2. í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œ ì „ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_korean_legal_text(text):\n",
    "    \"\"\"\n",
    "    í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œì˜ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        text (str): ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸\n",
    "        \n",
    "    ì£¼ìš” ì²˜ë¦¬ ë‚´ìš©:\n",
    "        - ë¶ˆí•„ìš”í•œ ê³µë°± ë° ê°œí–‰ ì •ë¦¬\n",
    "        - ë²•ì¡°ë¬¸ ë²ˆí˜¸ ì •ê·œí™” (ì œ1ì¡°, ì œ2ì¡° ë“±)\n",
    "        - í•­ ë²ˆí˜¸ë¥¼ ì•„ë¼ë¹„ì•„ ìˆ«ìë¡œ ë³€í™˜ (â‘ â†’ì œ1í•­)\n",
    "        - í˜¸ ë²ˆí˜¸ ì •ê·œí™”\n",
    "    \"\"\"\n",
    "    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ í†µì¼\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # ì¡°í•­ ë²ˆí˜¸ ì •ê·œí™”: \"ì œ 1 ì¡°\" -> \"ì œ1ì¡°\" í˜•íƒœë¡œ í†µì¼\n",
    "    text = re.sub(r'ì œ(\\d+)ì¡°', r'ì œ\\1ì¡°', text)\n",
    "    \n",
    "    # ì›ë¬¸ì í•­ ë²ˆí˜¸ë¥¼ ì•„ë¼ë¹„ì•„ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "    # â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘© -> ì œ1í•­, ì œ2í•­, ... ì œ10í•­\n",
    "    text = re.sub(r'â‘ |â‘¡|â‘¢|â‘£|â‘¤|â‘¥|â‘¦|â‘§|â‘¨|â‘©', \n",
    "                  lambda m: f\"ì œ{ord(m.group()) - ord('â‘ ') + 1}í•­\", text)\n",
    "    \n",
    "    # í˜¸ ë²ˆí˜¸ ì •ê·œí™”: \"1. \" -> \"ì œ1í˜¸ \" í˜•íƒœë¡œ ë³€í™˜\n",
    "    text = re.sub(r'(\\d+)\\.\\s', r'ì œ\\1í˜¸ ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. ë²•ë¥  ë¬¸ì„œì— ìµœì í™”ëœ í…ìŠ¤íŠ¸ ë¶„í•  í•¨ìˆ˜\n",
    "def advanced_split_text(text, chunk_size=600, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    ë²•ë¥  ë¬¸ì„œì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ ê³ ë ¤í•œ ì§€ëŠ¥ì  í…ìŠ¤íŠ¸ ë¶„í• \n",
    "    \n",
    "    Args:\n",
    "        text (str): ë¶„í• í•  í…ìŠ¤íŠ¸\n",
    "        chunk_size (int): ê° ì²­í¬ì˜ ëª©í‘œ í¬ê¸° (ë¬¸ì ìˆ˜)\n",
    "        chunk_overlap (int): ì²­í¬ ê°„ ì¤‘ë³µë˜ëŠ” ë¬¸ì ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        list: ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "    íŠ¹ì§•:\n",
    "        - ë²•ë¥  ë¬¸ì„œì˜ ê³„ì¸µ êµ¬ì¡°(ì¡°>í•­>í˜¸>ëª©)ë¥¼ ê³ ë ¤í•œ ë¶„í•  ìš°ì„ ìˆœìœ„\n",
    "        - ì˜ë¯¸ì  ì™„ì„±ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë¶„í• \n",
    "        - í† í° í•œë„ë¥¼ ê³ ë ¤í•œ ì ì ˆí•œ í¬ê¸° ì„¤ì •\n",
    "    \"\"\"\n",
    "    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "    text = preprocess_korean_legal_text(text)\n",
    "    \n",
    "    # ë²•ë¥  ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë¶„í•  êµ¬ë¶„ìë“¤ì„ ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ì„¤ì •\n",
    "    # ìƒìœ„ êµ¬ì¡°ë¶€í„° í•˜ìœ„ êµ¬ì¡° ìˆœì„œë¡œ ë¶„í• ì„ ì‹œë„\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\n",
    "            \"\\nì œ\", \"\\n**ì œ\",  # ì¡°í•­ ë‹¨ìœ„ ë¶„í•  (ê°€ì¥ ìš°ì„ )\n",
    "            \"\\nâ‘ \", \"\\nâ‘¡\", \"\\nâ‘¢\", \"\\nâ‘£\", \"\\nâ‘¤\",  # í•­ ë‹¨ìœ„ ë¶„í• \n",
    "            \"\\n1.\", \"\\n2.\", \"\\n3.\", \"\\n4.\", \"\\n5.\",  # í˜¸ ë‹¨ìœ„ ë¶„í• \n",
    "            \"\\nê°€.\", \"\\në‚˜.\", \"\\në‹¤.\", \"\\në¼.\", \"\\në§ˆ.\",  # ëª© ë‹¨ìœ„ ë¶„í• \n",
    "            \"\\n\\n\",  # ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„í• \n",
    "            \"\\n\",    # ì¤„ ë‹¨ìœ„ ë¶„í• \n",
    "            \". \",    # ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• \n",
    "            \" \",     # ë‹¨ì–´ ë‹¨ìœ„ ë¶„í• \n",
    "            \"\"       # ë¬¸ì ë‹¨ìœ„ ë¶„í•  (ìµœí›„ ìˆ˜ë‹¨)\n",
    "        ]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# 4. DOCX íŒŒì¼ ë¡œë”© ë° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def load_docx_advanced(file_path):\n",
    "    \"\"\"\n",
    "    DOCX íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì •ë¦¬ë¥¼ ìˆ˜í–‰\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): DOCX íŒŒì¼ ê²½ë¡œ\n",
    "        \n",
    "    Returns:\n",
    "        str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ ì‹œ\n",
    "        ValueError: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Docx2txtLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì •ë¦¬ ì‘ì—…\n",
    "        # ì—°ì†ëœ ë¹ˆ ì¤„ì„ ë‘ ê°œì˜ ì¤„ë°”ê¿ˆìœ¼ë¡œ í†µì¼\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # ì—°ì†ëœ ê³µë°±ê³¼ íƒ­ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ í†µì¼\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# 5. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ ë²¡í„° ì €ì¥ì†Œ ìƒì„± í•¨ìˆ˜\n",
    "def create_vector_store(text_chunks, embedding_model, batch_size=30):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "    í† í° í•œë„ ì´ˆê³¼ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ ì ìš©\n",
    "    \n",
    "    Args:\n",
    "        text_chunks (list): ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ë“¤\n",
    "        embedding_model: OpenAI ì„ë² ë”© ëª¨ë¸ ê°ì²´\n",
    "        batch_size (int): í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ìˆ˜ (ê¸°ë³¸ê°’: 30)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (FAISS ë²¡í„° ì €ì¥ì†Œ, Document ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸)\n",
    "        \n",
    "    ì²˜ë¦¬ ê³¼ì •:\n",
    "        1. ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ID, ê¸¸ì´, ì¡°í•­ ì •ë³´ ë“±)\n",
    "        2. ì²­í¬ í¬ê¸°ê°€ ë„ˆë¬´ í° ê²½ìš° ìë™ìœ¼ë¡œ ì œí•œ\n",
    "        3. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±í•˜ì—¬ í† í° í•œë„ ë¬¸ì œ ë°©ì§€\n",
    "        4. FAISS merge ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ë°°ì¹˜ë³„ ê²°ê³¼ í†µí•©\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"   ì´ {len(text_chunks)}ê°œ ì²­í¬ë¥¼ {batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬...\")\n",
    "        \n",
    "        # Document ê°ì²´ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "        documents = []\n",
    "        \n",
    "        # ê° í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜í•˜ë©´ì„œ ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            # ì²­í¬ í¬ê¸°ê°€ ë„ˆë¬´ í° ê²½ìš° ì œí•œ (í† í° í•œë„ ë°©ì§€)\n",
    "            if len(chunk) > 2000:\n",
    "                chunk = chunk[:2000] + \"...\"\n",
    "                print(f\"   ê²½ê³ : ì²­í¬ {i}ê°€ ë„ˆë¬´ ì»¤ì„œ 2000ìë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "            # ê° ì²­í¬ì— ì¶”ê°€í•  ë©”íƒ€ë°ì´í„° êµ¬ì„±\n",
    "            metadata = {\n",
    "                'chunk_id': i,  # ì²­í¬ ê³ ìœ  ë²ˆí˜¸\n",
    "                'chunk_length': len(chunk),  # ì²­í¬ ê¸¸ì´\n",
    "                'chunk_type': 'legal_document'  # ë¬¸ì„œ ìœ í˜•\n",
    "            }\n",
    "            \n",
    "            # ì²­í¬ ë‚´ìš©ì—ì„œ ì¡°í•­ ì •ë³´ ìë™ ì¶”ì¶œ\n",
    "            # \"ì œnì¡°\" íŒ¨í„´ì„ ì°¾ì•„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€\n",
    "            if 'ì œ' in chunk and 'ì¡°' in chunk:\n",
    "                article_match = re.search(r'ì œ(\\d+)ì¡°', chunk)\n",
    "                if article_match:\n",
    "                    metadata['article'] = f\"ì œ{article_match.group(1)}ì¡°\"\n",
    "            \n",
    "            # Document ê°ì²´ ìƒì„±í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "            documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "        \n",
    "        # ë°°ì¹˜ë³„ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ë³‘í•© ê³¼ì •\n",
    "        vector_store = None\n",
    "        total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # ë¬¸ì„œë“¤ì„ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch_docs = documents[i:i + batch_size]\n",
    "            batch_num = (i // batch_size) + 1\n",
    "            \n",
    "            print(f\"   ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_docs)}ê°œ ë¬¸ì„œ)\")\n",
    "            \n",
    "            # ì²« ë²ˆì§¸ ë°°ì¹˜ì¸ ê²½ìš° ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "            if vector_store is None:\n",
    "                vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "            else:\n",
    "                # ì´í›„ ë°°ì¹˜ë“¤ì€ ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œì— ë³‘í•©\n",
    "                batch_vector_store = FAISS.from_documents(batch_docs, embedding_model)\n",
    "                vector_store.merge_from(batch_vector_store)\n",
    "        \n",
    "        print(\"   ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "        return vector_store, documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# 6. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def keyword_search(query, documents, k=5):\n",
    "    \"\"\"\n",
    "    ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    ë²¡í„° ê²€ìƒ‰ê³¼ ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ\n",
    "    \n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰ ì§ˆì˜\n",
    "        documents (list): Document ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        k (int): ë°˜í™˜í•  ìƒìœ„ ë¬¸ì„œ ìˆ˜\n",
    "        \n",
    "    Returns:\n",
    "        list: ê´€ë ¨ë„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ Document ê°ì²´ë“¤\n",
    "        \n",
    "    ê²€ìƒ‰ ë¡œì§:\n",
    "        1. ì§ˆì˜ì™€ ë¬¸ì„œì˜ ë‹¨ì–´ êµì§‘í•© ê³„ì‚°\n",
    "        2. êµì§‘í•© í¬ê¸°ë¥¼ ì§ˆì˜ ë‹¨ì–´ ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì •ê·œí™”\n",
    "        3. ì •í™•í•œ êµ¬ë¬¸ ë§¤ì¹­ ì‹œ ë³´ë„ˆìŠ¤ ì ìˆ˜ ë¶€ì—¬\n",
    "        4. ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ kê°œ ë¬¸ì„œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    # ì§ˆì˜ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # ê° ë¬¸ì„œì˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ë¦¬ìŠ¤íŠ¸\n",
    "    scores = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # ë¬¸ì„œ ë‚´ìš©ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "        content_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # ì§ˆì˜ ë‹¨ì–´ì™€ ë¬¸ì„œ ë‹¨ì–´ì˜ êµì§‘í•© ê³„ì‚°\n",
    "        intersection = query_words.intersection(content_words)\n",
    "        \n",
    "        # ê¸°ë³¸ ì ìˆ˜: êµì§‘í•© í¬ê¸°ë¥¼ ì§ˆì˜ ë‹¨ì–´ ìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì •ê·œí™” (0~1 ë²”ìœ„)\n",
    "        score = len(intersection) / len(query_words) if query_words else 0\n",
    "        \n",
    "        # ë³´ë„ˆìŠ¤ ì ìˆ˜: ì§ˆì˜ ì „ì²´ê°€ ë¬¸ì„œì— ì •í™•íˆ í¬í•¨ëœ ê²½ìš°\n",
    "        if query.lower() in doc.page_content.lower():\n",
    "            score += 0.5\n",
    "        \n",
    "        # (ì ìˆ˜, ì¸ë±ìŠ¤, ë¬¸ì„œ) íŠœí”Œë¡œ ì €ì¥\n",
    "        scores.append((score, i, doc))\n",
    "    \n",
    "    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, _, doc in scores[:k]]\n",
    "\n",
    "# 7. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜ (ë²¡í„° + í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©)\n",
    "def hybrid_search(query, vector_store, documents, k=5, alpha=0.7):\n",
    "    \"\"\"\n",
    "    ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰\n",
    "    ë‘ ê²€ìƒ‰ ë°©ë²•ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ ì œê³µ\n",
    "    \n",
    "    Args:\n",
    "        query (str): ê²€ìƒ‰ ì§ˆì˜\n",
    "        vector_store: FAISS ë²¡í„° ì €ì¥ì†Œ\n",
    "        documents (list): Document ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        k (int): ìµœì¢… ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜\n",
    "        alpha (float): ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (0~1, ë†’ì„ìˆ˜ë¡ ë²¡í„° ê²€ìƒ‰ ì¤‘ì‹œ)\n",
    "        \n",
    "    Returns:\n",
    "        list: ì¢…í•© ì ìˆ˜ë¡œ ì •ë ¬ëœ ìƒìœ„ kê°œ Document ê°ì²´ë“¤\n",
    "        \n",
    "    ê²€ìƒ‰ ê³¼ì •:\n",
    "        1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ëœ ë¬¸ì„œ ì°¾ê¸°\n",
    "        2. í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•í•œ ìš©ì–´ ë§¤ì¹­ ë¬¸ì„œ ì°¾ê¸°\n",
    "        3. ë‘ ê²°ê³¼ë¥¼ alpha ê°€ì¤‘ì¹˜ë¡œ ê²°í•©\n",
    "        4. ì¤‘ë³µ ë¬¸ì„œ ì²˜ë¦¬ ë° ìµœì¢… ì ìˆ˜ ê³„ì‚°\n",
    "        5. ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    # ë” ë§ì€ í›„ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ë‹¤ì–‘ì„± í™•ë³´ (k*2ê°œ)\n",
    "    vector_results = vector_store.similarity_search(query, k=k*2)\n",
    "    \n",
    "    # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    keyword_results = keyword_search(query, documents, k=k*2)\n",
    "    \n",
    "    # 3. ë‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì ìˆ˜ì™€ í•¨ê»˜ í†µí•©\n",
    "    combined_results = {}\n",
    "    \n",
    "    # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì— ì ìˆ˜ ë¶€ì—¬ (alpha ê°€ì¤‘ì¹˜ ì ìš©)\n",
    "    for i, doc in enumerate(vector_results):\n",
    "        # ë¬¸ì„œ ë‚´ìš©ì„ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš©\n",
    "        doc_id = doc.page_content\n",
    "        # ìˆœìœ„ê°€ ë†’ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ (1.0ì—ì„œ ì‹œì‘í•˜ì—¬ ìˆœìœ„ì— ë”°ë¼ ê°ì†Œ)\n",
    "        vector_score = alpha * (1.0 - i / len(vector_results))\n",
    "        \n",
    "        combined_results[doc_id] = {\n",
    "            'document': doc,\n",
    "            'score': vector_score,\n",
    "            'vector_rank': i + 1,\n",
    "            'keyword_rank': None\n",
    "        }\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì— ì ìˆ˜ ë¶€ì—¬ ((1-alpha) ê°€ì¤‘ì¹˜ ì ìš©)\n",
    "    for i, doc in enumerate(keyword_results):\n",
    "        doc_id = doc.page_content\n",
    "        keyword_score = (1 - alpha) * (1.0 - i / len(keyword_results))\n",
    "        \n",
    "        if doc_id in combined_results:\n",
    "            # ì´ë¯¸ ë²¡í„° ê²€ìƒ‰ì—ì„œ ì°¾ì€ ë¬¸ì„œì¸ ê²½ìš° ì ìˆ˜ í•©ì‚°\n",
    "            combined_results[doc_id]['score'] += keyword_score\n",
    "            combined_results[doc_id]['keyword_rank'] = i + 1\n",
    "        else:\n",
    "            # í‚¤ì›Œë“œ ê²€ìƒ‰ì—ì„œë§Œ ì°¾ì€ ìƒˆë¡œìš´ ë¬¸ì„œì¸ ê²½ìš° ì¶”ê°€\n",
    "            combined_results[doc_id] = {\n",
    "                'document': doc,\n",
    "                'score': keyword_score,\n",
    "                'vector_rank': None,\n",
    "                'keyword_rank': i + 1\n",
    "            }\n",
    "    \n",
    "    # 4. ì¢…í•© ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜\n",
    "    sorted_results = sorted(combined_results.values(), key=lambda x: x['score'], reverse=True)\n",
    "    return [result['document'] for result in sorted_results[:k]]\n",
    "\n",
    "# 8. í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œ ì „ìš© í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "def create_korean_legal_prompt():\n",
    "    \"\"\"\n",
    "    í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œ íŠ¹ì„±ì— ë§ì¶˜ ì „ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "    \n",
    "    Returns:\n",
    "        PromptTemplate: ë²•ë¥  ë¬¸ì„œ ì§ˆì˜ì‘ë‹µì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "        \n",
    "    í”„ë¡¬í”„íŠ¸ íŠ¹ì§•:\n",
    "        - ë²•ì¡°ë¬¸ ì¸ìš©ì˜ ì •í™•ì„± ê°•ì¡°\n",
    "        - ì „ë¬¸ ìš©ì–´ì— ëŒ€í•œ ì‰¬ìš´ ì„¤ëª… ìš”êµ¬\n",
    "        - ì¡°í•­ ê°„ ì—°ê´€ì„± ì„¤ëª… í¬í•¨\n",
    "        - ì‹¤ë¬´ì  ì ìš© ë°©ë²• ì œì‹œ\n",
    "        - ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì— ëŒ€í•œ ëª…ì‹œì  ì–¸ê¸‰\n",
    "    \"\"\"\n",
    "    template = \"\"\"ë‹¹ì‹ ì€ í•œêµ­ ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë²•ë¥  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:\n",
    "1. ë²•ì¡°ë¬¸ì˜ ì¡°í•­, í•­, í˜¸, ëª©ì„ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”\n",
    "2. ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ì‰¬ìš´ ì„¤ëª…ì„ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”\n",
    "3. ê´€ë ¨ ì¡°í•­ë“¤ ê°„ì˜ ì—°ê´€ì„±ì„ ì„¤ëª…í•˜ì„¸ìš”\n",
    "4. ì‹¤ë¬´ì  ì ìš© ë°©ë²•ë„ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”\n",
    "5. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì„¸ìš”\n",
    "\n",
    "ì°¸ê³  ë¬¸ì„œ:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ìœ„ ë²•ë¥  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. ê´€ë ¨ ì¡°í•­ì„ ì¸ìš©í•˜ë©° ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\"\"\n",
    "\n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "# 9. LLMì„ í™œìš©í•œ ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜\n",
    "def query_with_llm(query, vector_store, documents):\n",
    "    \"\"\"\n",
    "    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê³¼ ê³ ì„±ëŠ¥ LLMì„ ê²°í•©í•œ ì§ˆë¬¸ ì‘ë‹µ ì‹œìŠ¤í…œ\n",
    "    \n",
    "    Args:\n",
    "        query (str): ì‚¬ìš©ì ì§ˆë¬¸\n",
    "        vector_store: FAISS ë²¡í„° ì €ì¥ì†Œ\n",
    "        documents (list): Document ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        dict: ë‹µë³€, ì°¸ê³  ë¬¸ì„œ, ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‘ë‹µ ë”•ì…”ë„ˆë¦¬\n",
    "        \n",
    "    ì²˜ë¦¬ ê³¼ì •:\n",
    "        1. GPT-4o-mini ëª¨ë¸ë¡œ LLM ì´ˆê¸°í™” (ë†’ì€ ì •í™•ë„)\n",
    "        2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ 7ê°œ ê²€ìƒ‰\n",
    "        3. ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©\n",
    "        4. ë²•ë¥  ë¬¸ì„œ ì „ìš© í”„ë¡¬í”„íŠ¸ ì ìš©\n",
    "        5. LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # LLM ëª¨ë¸ ì„¤ì •\n",
    "        llm = ChatUpstage(\n",
    "            model=\"solar-pro\",\n",
    "            base_url=\"https://api.upstage.ai/v1\",\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        print(f\"ì§ˆì˜: {query}\")\n",
    "        print(\"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...\")\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
    "        # k=7ë¡œ ì„¤ì •í•˜ì—¬ ì¶©ë¶„í•œ ì»¨í…ìŠ¤íŠ¸ í™•ë³´\n",
    "        # alpha=0.7ë¡œ ì„¤ì •í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ì„ ë” ì¤‘ì‹œ (ì˜ë¯¸ì  ìœ ì‚¬ë„ ìš°ì„ )\n",
    "        relevant_docs = hybrid_search(query, vector_store, documents, k=7, alpha=0.7)\n",
    "        \n",
    "        print(f\"ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ: {len(relevant_docs)}ê°œ\")\n",
    "        \n",
    "        # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©\n",
    "        # ê° ë¬¸ì„œì— ë²ˆí˜¸ë¥¼ ë§¤ê²¨ êµ¬ë¶„í•˜ê¸° ì‰½ê²Œ êµ¬ì„±\n",
    "        context = \"\\n\\n\".join([f\"[ë¬¸ì„œ {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        # í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©\n",
    "        prompt = create_korean_legal_prompt()\n",
    "        \n",
    "        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ ì‚½ì…)\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        print(\"LLM ì‘ë‹µ ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # LLMì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜\n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"source_documents\": relevant_docs,\n",
    "            \"context_used\": context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# 10. ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate_context_quality(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ í’ˆì§ˆì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€\n",
    "    \n",
    "    Args:\n",
    "        query (str): ì›ë³¸ ì§ˆì˜\n",
    "        retrieved_docs (list): ê²€ìƒ‰ëœ Document ê°ì²´ë“¤\n",
    "        \n",
    "    Returns:\n",
    "        float: 0~1 ë²”ìœ„ì˜ í’ˆì§ˆ ì ìˆ˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ í’ˆì§ˆ)\n",
    "        \n",
    "    í‰ê°€ ê¸°ì¤€:\n",
    "        1. í‚¤ì›Œë“œ ë§¤ì¹­ë¥  (70% ê°€ì¤‘ì¹˜): ì§ˆì˜ ë‹¨ì–´ê°€ ë¬¸ì„œì— í¬í•¨ëœ ë¹„ìœ¨\n",
    "        2. ë¬¸ì„œ ê¸¸ì´ ì ì ˆì„± (30% ê°€ì¤‘ì¹˜): ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ ì ì ˆí•œ ê¸¸ì´\n",
    "    \"\"\"\n",
    "    # ì§ˆì˜ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  ì†Œë¬¸ì ë³€í™˜\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # ê° ë¬¸ì„œë³„ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n",
    "    quality_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        # ë¬¸ì„œ ë‚´ìš©ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  ì†Œë¬¸ì ë³€í™˜\n",
    "        doc_words = set(doc.page_content.lower().split())\n",
    "        \n",
    "        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°\n",
    "        # ì§ˆì˜ ë‹¨ì–´ ì¤‘ ë¬¸ì„œì— í¬í•¨ëœ ë‹¨ì–´ì˜ ë¹„ìœ¨\n",
    "        keyword_match = len(query_words.intersection(doc_words)) / len(query_words)\n",
    "        \n",
    "        # ë¬¸ì„œ ê¸¸ì´ ì ìˆ˜ ê³„ì‚°\n",
    "        # 1000ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” (1000ì ì´ìƒì´ë©´ 1.0ì )\n",
    "        length_score = min(len(doc.page_content) / 1000, 1.0)\n",
    "        \n",
    "        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (í‚¤ì›Œë“œ ë§¤ì¹­ 70% + ê¸¸ì´ ì ì ˆì„± 30%)\n",
    "        total_score = (keyword_match * 0.7) + (length_score * 0.3)\n",
    "        quality_scores.append(total_score)\n",
    "    \n",
    "    # ì „ì²´ ë¬¸ì„œì˜ í‰ê·  í’ˆì§ˆ ì ìˆ˜ ë°˜í™˜\n",
    "    avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "    return avg_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„\n",
    "if __name__ == \"__main__\":\n",
    "    # ì²˜ë¦¬í•  DOCX íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    docx_path = \"data/tax_with_table.docx\"\n",
    "    \n",
    "    print(\"ê°œì„ ëœ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë“œ\n",
    "    print(\"1. ë¬¸ì„œ ë¡œë“œ ì¤‘...\")\n",
    "    text = load_docx_advanced(docx_path)\n",
    "    print(f\"   ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(text):,} ë¬¸ì\")\n",
    "    \n",
    "    # 2ë‹¨ê³„: ë¬¸ì„œ ë¶„í• \n",
    "    print(\"\\n2. ë¬¸ì„œ ë¶„í•  ì¤‘...\")\n",
    "    text_chunks = advanced_split_text(text, chunk_size=600, chunk_overlap=100)\n",
    "    print(f\"   ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(text_chunks)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "    \n",
    "    # ì²­í¬ í¬ê¸° í†µê³„ ë¶„ì„ ë° ì¶œë ¥\n",
    "    chunk_lengths = [len(chunk) for chunk in text_chunks]\n",
    "    avg_length = sum(chunk_lengths) / len(chunk_lengths)\n",
    "    max_length = max(chunk_lengths)\n",
    "    print(f\"   í‰ê·  ì²­í¬ ê¸¸ì´: {avg_length:.0f}ì, ìµœëŒ€ ê¸¸ì´: {max_length}ì\")\n",
    "    \n",
    "    # 3ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    print(\"\\n3. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”...\")\n",
    "    \n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "    )\n",
    "    print(\"   ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "    \n",
    "    # 4ë‹¨ê³„: ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "    print(\"\\n4. ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...\")\n",
    "    # ë°°ì¹˜ í¬ê¸° 30ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í† í° í•œë„ ë¬¸ì œ ë°©ì§€\n",
    "    vector_store, documents = create_vector_store(text_chunks, embedding_model, batch_size=30)\n",
    "    print(\"   ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    # 5ë‹¨ê³„: ì§ˆì˜ ì‹¤í–‰\n",
    "    print(\"\\n5. ì§ˆì˜ ì‹¤í–‰ ì¤‘...\")\n",
    "   #query = \"ì´ìˆ˜ì…ê¸ˆì•¡ ë¶ˆì‚°ì…ì— ëŒ€í•˜ì—¬ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"\n",
    "    query = \"ë¹„ê³¼ì„¸ì†Œë“ì˜ ì¢…ë¥˜ì— ëŒ€í•˜ì—¬ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6ë‹¨ê³„: ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7ë‹¨ê³„: ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AIì˜ ë‹µë³€:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(results['source_documents'])}ê°œ\")\n",
    "    print(f\"ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜: {context_quality:.2f}/1.00\")\n",
    "    print(f\"ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(results['context_used']):,} ë¬¸ì\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ì°¸ê³ í•œ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(\"=\" * 60)\n",
    "    # ìƒìœ„ 3ê°œ ë¬¸ì„œì˜ ì¼ë¶€ë§Œ ë¯¸ë¦¬ë³´ê¸°ë¡œ ì¶œë ¥\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200ìê¹Œì§€ë§Œ ë¯¸ë¦¬ë³´ê¸°ë¡œ í‘œì‹œ\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[ë¬¸ì„œ {i+1}] {preview}\")\n",
    "            # 5ë‹¨ê³„: ì§ˆì˜ ì‹¤í–‰\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n5. ì§ˆì˜ ì‹¤í–‰ ì¤‘...\")\n",
    "    query = \"ì´ìˆ˜ì…ê¸ˆì•¡ ë¶ˆì‚°ì…ì— ëŒ€í•˜ì—¬ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"\n",
    "    results = query_with_llm(query, vector_store, documents)\n",
    "    \n",
    "    # 6ë‹¨ê³„: ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€\n",
    "    context_quality = evaluate_context_quality(query, results[\"source_documents\"])\n",
    "    \n",
    "    # 7ë‹¨ê³„: ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AIì˜ ë‹µë³€:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results[\"answer\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(results['source_documents'])}ê°œ\")\n",
    "    print(f\"ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜: {context_quality:.2f}/1.00\")\n",
    "    print(f\"ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(results['context_used']):,} ë¬¸ì\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ì°¸ê³ í•œ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(\"=\" * 60)\n",
    "    # ìƒìœ„ 3ê°œ ë¬¸ì„œì˜ ì¼ë¶€ë§Œ ë¯¸ë¦¬ë³´ê¸°ë¡œ ì¶œë ¥\n",
    "    for i, doc in enumerate(results[\"source_documents\"][:3]):\n",
    "        # 200ìê¹Œì§€ë§Œ ë¯¸ë¦¬ë³´ê¸°ë¡œ í‘œì‹œ\n",
    "        preview = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "        print(f\"\\n[ë¬¸ì„œ {i+1}] {preview}\")\n",
    "    print(\"=\" * 60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
