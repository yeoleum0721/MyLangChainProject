{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0f96f1-910f-438f-876f-9eff119c2b0a",
   "metadata": {
    "id": "ed0f96f1-910f-438f-876f-9eff119c2b0a"
   },
   "source": [
    "### LLM Chain 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b1e2f-9d67-4d01-9a8c-83ced6b711a9",
   "metadata": {
    "id": "5a5b1e2f-9d67-4d01-9a8c-83ced6b711a9"
   },
   "source": [
    "## 1. 환경 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b7854-f96a-47fc-b3c7-b2bdfb55df81",
   "metadata": {
    "id": "b03b7854-f96a-47fc-b3c7-b2bdfb55df81"
   },
   "source": [
    "### 1) 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cd87a33-0a37-461b-8f37-3c142e60b1f6",
    "outputId": "c96ed02d-19b7-4e90-d92e-1ae52895e303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  - python-dotenv\n",
      "  - langchain\n",
      "  - langchain-openai\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "! poetry add python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55152049-e9e5-4952-8e19-409f58cf3ac9",
   "metadata": {
    "id": "55152049-e9e5-4952-8e19-409f58cf3ac9"
   },
   "source": [
    "### 2) OpenAI 인증키 설정\n",
    "https://openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76f68a8-4745-4377-8057-6090b87377d1",
   "metadata": {
    "id": "b76f68a8-4745-4377-8057-6090b87377d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_G\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09aaca6-5aa2-4b52-bbfc-196e808dc5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.27\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01c50a-32cf-49af-891a-f9b17fa0bd6c",
   "metadata": {
    "id": "fc01c50a-32cf-49af-891a-f9b17fa0bd6c"
   },
   "source": [
    "## 2. LLM Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23729d10-9600-415b-b7d1-f954665224e3",
   "metadata": {
    "id": "23729d10-9600-415b-b7d1-f954665224e3"
   },
   "source": [
    "### 1) Prompt + LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "on0y4xF8VoyE",
   "metadata": {
    "id": "on0y4xF8VoyE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# chain 실행\n",
    "result = llm.invoke(\"인공지능 모델의 학습 원리에 대하여 쉽게 설명해 주세요.\")\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc4accf-c927-40a3-ba2c-f891c94c34f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='인공지능 모델의 학습 원리는 크게 세 가지로 설명할 수 있습니다.\\n\\n*   **데이터 수집 및 준비** 학습을 위해서는 인공지능 모델이 학습할 수 있는 충분한 양의 데이터가 필요합니다. 데이터는 이미지, 텍스트, 오디오 등 다양한 형태가 있을 수 있습니다. 수집된 데이터는 정제되고, 라벨링되는 과정을 거쳐 모델이 학습할 수 있는 형태로 준비됩니다. 예를 들어, 고양이와 강아지의 사진을 분류하는 모델을 학습할 때는 고양이와 강아지의 사진 데이터와 각 사진에 고양이 또는 강아지라는 라벨을 준비합니다.\\n*   **모델 훈련** 준비된 데이터를 바탕으로 인공지능 모델을 훈련합니다. 모델은 입력된 데이터와 예상 출력 간의 차이를 계산하고, 이 차이(오차)를 줄이기 위해 모델의 가중치를 조정하는 과정을 반복합니다. 이 과정은 최적화 알고리즘에 의해 수행되며, 모델이 데이터를 얼마나 잘 학습했는지를 나타내는 손실 함수를 최소화하는 방향으로 모델이 업데이트됩니다.\\n*   **모델 평가 및 개선** 훈련된 모델의 성능을 평가하기 위해 테스트 데이터를 사용합니다. 테스트 데이터에 대한 모델의 성능을 측정하고, 필요하다면 모델의 구조를 조정하거나 학습 데이터를 추가하는 등의 방법으로 모델을 개선합니다.\\n\\n예를 들어, 간단한 선형 회귀 모델을 생각해 보겠습니다. 선형 회귀 모델은 직선을 이용하여 데이터 포인트들을 가장 잘 표현하려고 합니다. 이 모델은 주어진 데이터 포인트와 모델이 예측한 값 사이의 오차를 계산하고, 오차를 줄이기 위해 직선의 기울기와 절편을 조정합니다. 이 과정은 오차가 충분히 작아질 때까지 반복되며, 모델은 데이터의 패턴을 학습하게 됩니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 349, 'prompt_tokens': 24, 'total_tokens': 373, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.213868849, 'prompt_time': 0.000531352, 'completion_time': 0.822891251, 'total_time': 0.823422603}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-5193e329-e653-4969-937a-579cbcd6c979', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--61691b77-1449-4630-b36c-7b5797250838-0' usage_metadata={'input_tokens': 24, 'output_tokens': 349, 'total_tokens': 373, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "WzcZy4PruV1n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "WzcZy4PruV1n",
    "outputId": "18ecc8f9-5748-4b16-cb07-4a0f7a01fb5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능 모델의 학습 원리는 크게 세 가지로 설명할 수 있습니다.\n",
      "\n",
      "*   **데이터 수집 및 준비** 학습을 위해서는 인공지능 모델이 학습할 수 있는 충분한 양의 데이터가 필요합니다. 데이터는 이미지, 텍스트, 오디오 등 다양한 형태가 있을 수 있습니다. 수집된 데이터는 정제되고, 라벨링되는 과정을 거쳐 모델이 학습할 수 있는 형태로 준비됩니다. 예를 들어, 고양이와 강아지의 사진을 분류하는 모델을 학습할 때는 고양이와 강아지의 사진 데이터와 각 사진에 고양이 또는 강아지라는 라벨을 준비합니다.\n",
      "*   **모델 훈련** 준비된 데이터를 바탕으로 인공지능 모델을 훈련합니다. 모델은 입력된 데이터와 예상 출력 간의 차이를 계산하고, 이 차이(오차)를 줄이기 위해 모델의 가중치를 조정하는 과정을 반복합니다. 이 과정은 최적화 알고리즘에 의해 수행되며, 모델이 데이터를 얼마나 잘 학습했는지를 나타내는 손실 함수를 최소화하는 방향으로 모델이 업데이트됩니다.\n",
      "*   **모델 평가 및 개선** 훈련된 모델의 성능을 평가하기 위해 테스트 데이터를 사용합니다. 테스트 데이터에 대한 모델의 성능을 측정하고, 필요하다면 모델의 구조를 조정하거나 학습 데이터를 추가하는 등의 방법으로 모델을 개선합니다.\n",
      "\n",
      "예를 들어, 간단한 선형 회귀 모델을 생각해 보겠습니다. 선형 회귀 모델은 직선을 이용하여 데이터 포인트들을 가장 잘 표현하려고 합니다. 이 모델은 주어진 데이터 포인트와 모델이 예측한 값 사이의 오차를 계산하고, 오차를 줄이기 위해 직선의 기울기와 절편을 조정합니다. 이 과정은 오차가 충분히 작아질 때까지 반복되며, 모델은 데이터의 패턴을 학습하게 됩니다.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c97fc",
   "metadata": {},
   "source": [
    "### 2) PromptTemplate + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "SeNi_VXqYD-b",
   "metadata": {
    "id": "SeNi_VXqYD-b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "input_variables=['input'] input_types={} partial_variables={} template='You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01WLucSpYjZt",
   "metadata": {
    "id": "01WLucSpYjZt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. 컴퓨터가 데이터를 통해 스스로 학습할 수 있도록 하는 것입니다. \\n\\n1. **데이터 수집**: 우선 인공지능이 학습할 데이터를 수집합니다. 이 데이터는 과거에 발생했던 일에 대한 정보이거나, 현실에서 관찰된 정보일 수 있습니다.\\n\\n2. **데이터 전처리**: 수집한 데이터를 분석에 적합한 형태로 가공하는 과정입니다. 중복된 데이터를 제거하거나, 결측치를 보완하는 등의 작업이 이에 포함됩니다.\\n\\n3. **모델 선택**: 인공지능 모델에는 여러 종류가 있습니다. 수집한 데이터의 특성과 해결하고자 하는 문제에 적합한 모델을 선택합니다. 예를 들어, 이미지 인식에는 합성곱 신경망(CNN), 자연어 처리에는 순환 신경망(RNN)이나 트랜스포머 등이 사용됩니다.\\n\\n4. **학습**: 선택한 모델에 데이터를 입력하여 모델의 파라미터를 조정하는 과정입니다. 이 과정에서 모델은 입력 데이터와 그에 따른 결과(레이블)를 통해 스스로를 수정합니다. 이 과정은 '손실 함수(loss function)'를 통해 모델의 예측 오류를 최소화하는 방향으로 진행됩니다.\\n\\n5. **평가**: 학습된 모델의 성능을 평가합니다. 평가 데이터 세트를 사용하여 모델의 예측 성능을 측정하고, 필요에 따라 모델의 하이퍼파라미터를 조정하거나 학습을 반복합니다.\\n\\n6. **예측**: 학습과 평가를 통해 완성된 모델을 사용하여 새로운 데이터에 대한 예측이나 분류 작업을 수행합니다.\\n\\n예를 들어, 고양이와 강아지의 사진을 구분하는 모델을 만든다고 가정해 봅시다. \\n\\n- 데이터를 수집합니다. (고양이, 강아지 사진들)\\n- 전처리 과정을 거쳐 사진들을 분석에 적합한 형태로 가공합니다.\\n- 이미지 인식에 적합한 모델(CNN 등)을 선택합니다.\\n- 고양이/강아지 분류 문제에 대해 모델을 학습시킵니다. (고양이 사진에는 '고양이'라는 레이블을, 강아지 사진에는 '강아지'라는 레이블을 입력)\\n- 평가 데이터 세트로 모델의 성능을 평가하고 필요하면 학습을 반복합니다.\\n- 완성된 모델로 새로운 사진을 업로드하면, 모델은 해당 사진이 고양이인지 강아지인지 예측할 수 있습니다.\\n\\n이처럼 인공지능 모델은 주어진 데이터를 통해 학습하고, 이를 바탕으로 새로운 데이터에 대한 예측이나 판단을 내릴 수 있습니다.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 506, 'prompt_tokens': 36, 'total_tokens': 542, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.232323875, 'prompt_time': 0.000960044, 'completion_time': 1.258142764, 'total_time': 1.259102808}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-e9471941-8e00-424b-9006-c61ccc7116d1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--322efa0e-e282-4a8b-b4d7-3adf576103ba-0' usage_metadata={'input_tokens': 36, 'output_tokens': 506, 'total_tokens': 542, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "print(type(chain))\n",
    "\n",
    "# chain 호출\n",
    "result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ec878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56848a4c",
   "metadata": {},
   "source": [
    "### 3) PromptTemplate + LLM(invoke()) + StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. 컴포넌트 정의\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 2. chain 생성 (LCEL)\n",
    "chain = prompt | llm | output_parser\n",
    "print(type(chain))\n",
    "\n",
    "# 3. chain의 invoke 호출\n",
    "result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d997b16",
   "metadata": {},
   "source": [
    "### 4) PromptTemplate + LLM(stream()) + StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "684654e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 원리와 유사합니다. \n",
      "\n",
      "1. **데이터 수집**: 우선, 인공지능 모델이 학습할 수 있는 많은 데이터를 수집합니다. 이 데이터는 문제에 대한 다양한 사례와 그에 따른 결과로 구성됩니다.\n",
      "\n",
      "2. **데이터 전처리**: 수집된 데이터는 모델이 이해할 수 있는 형태로 변환됩니다. 예를 들어, 이미지 데이터는 픽셀 값으로 변환되고, 텍스트 데이터는 단어 벡터로 변환됩니다.\n",
      "\n",
      "3. **모델 초기화**: 인공지능 모델은 수학적 함수로 표현되며, 이 함수는 다양한 변수(파라미터)를 가지고 있습니다. 이 변수들은 모델의 초기값으로 설정됩니다.\n",
      "\n",
      "4. **예측**: 모델은 학습된 데이터를 바탕으로 예측을 수행합니다. 이 예측은 실제 결과와 비교하여 얼마나 정확한지 평가됩니다.\n",
      "\n",
      "5. **오차 계산**: 모델의 예측과 실제 결과 사이의 차이(오차)를 계산합니다. 이 오차는 모델이 얼마나 잘못 예측했는지를 나타냅니다.\n",
      "\n",
      "6. **변수 업데이트**: 오차를 줄이기 위해 모델의 변수들을 조정합니다. 이 과정은 모델이 실제 결과에 더 가까운 예측을 할 수 있도록 합니다.\n",
      "\n",
      "7. **반복**: 4, 5, 6 단계를 여러 번 반복합니다. 반복할수록 모델은 더 많은 데이터를 학습하고, 예측의 정확도가 높아집니다.\n",
      "\n",
      "8. **모델 평가**: 학습이 완료된 후, 모델의 성능을 평가합니다. 평가 데이터에 대한 모델의 예측 정확도를 확인하여 모델의 성능을 결정합니다.\n",
      "\n",
      "이를 통해 인공지능 모델은 주어진 데이터를 학습하고, 새로운 데이터에 대해서도 정확한 예측을 할 수 있게 됩니다."
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. 컴포넌트 정의\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert.\\\n",
    "                                       Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "lm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "# 스트리밍 출력\n",
    "#print(answer)\n",
    "\n",
    "for token in answer:\n",
    "    # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0188674-915f-41af-ac46-56f9f54289b0",
   "metadata": {
    "id": "b0188674-915f-41af-ac46-56f9f54289b0"
   },
   "source": [
    "##### 2) Multiple Chains\n",
    "* Multi Chain을 활용한 영화 추천 및 줄거리 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31678c55-38a8-4ca2-b437-9d4495946b0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "31678c55-38a8-4ca2-b437-9d4495946b0a",
    "outputId": "7ee83878-5d1a-45e3-f033-100da33f3ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      " ===> 추천된 영화:\n",
      "드라마 장르라면, _『아이 다니엘 블레이크』(2016)_를 꼭 권하고 싶습니다.  \n",
      "켄 로치 감독이 연출한 이 작품은 영국 노동계층의 삶을 냉정하면서도 따뜻하게 그려내며, “사람은 일을 하기 위해 사는 게 아니라, 사람답게 살기 위해 일한다”는 메시지를 강렬하게 던집니다. 예산은 적지만, 박장대소와 눈물을 오가는 몇 안 되는 영화죠. 끝 크레디트가 올라가도 한동안 허전함이 가시지 않을 겁니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추천한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 정보(제목,감독,캐스팅,줄거리)를 알려 주세요\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "movie = chain1.invoke({\"genre\": \"Drama\"})  # 영화 제목 얻기\n",
    "\n",
    "print(type(movie))\n",
    "print(\" ===> 추천된 영화:\")  # movie 값 출력\n",
    "print(movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16718b76-f59d-48f7-906f-5d2371417803",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "16718b76-f59d-48f7-906f-5d2371417803",
    "outputId": "6e3371cd-d294-4be7-a868-2eae5ee6e5de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  movie: ChatPromptTemplate(input_variables=['genre'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['genre'], input_types={}, partial_variables={}, template='{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.'), additional_kwargs={})])\n",
      "         | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F69A3AC440>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F69A3ADC10>, root_client=<openai.OpenAI object at 0x000001F69A3ADD60>, root_async_client=<openai.AsyncOpenAI object at 0x000001F69A3AC080>, model_name='moonshotai/kimi-k2-instruct-0905', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
      "         | StrOutputParser()\n",
      "} middle=[ChatPromptTemplate(input_variables=['movie'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['movie'], input_types={}, partial_variables={}, template='{movie} 추천한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 정보(제목,감독,캐스팅,줄거리)를 알려 주세요'), additional_kwargs={})]), ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F69A3AC440>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F69A3ADC10>, root_client=<openai.OpenAI object at 0x000001F69A3ADD60>, root_async_client=<openai.AsyncOpenAI object at 0x000001F69A3AC080>, model_name='moonshotai/kimi-k2-instruct-0905', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')] last=StrOutputParser()\n",
      "\n",
      "🔹 영화 줄거리 요약:\n",
      " 문라이트\n",
      "\n",
      "문라이트  \n",
      "감독: 바리 제니킨스  \n",
      "출연: 트레반테 로드(성년 치론), 애슈턴 샌더스(청소년 치론), 알렉스 히비(유년 치론), 마허샬라 알리(후안), 나오미 해리스(보울라), 안드레 홀랜드(케빈)  \n",
      "줄거리: 1980년대 마이애미의 자유아메리카 빈민가에서 태어난 소년 ‘치론’은 마약상 ‘후안’의 보호 아래 성장하며 자신의 정체성과 사랑, 고독을 차례로 마주한다. 유년·청소년·성년 세 시기로 나뉜 이야기는 흑인이자 동성애자이며 가난한 ‘치론’이 ‘나는 누구인가’라는 질문에 답을 찾아가는 여정을 담담하게 그려낸다.\n"
     ]
    }
   ],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "chain2 = (\n",
    "    {\"movie\": chain1}  # chain1의 출력을 movie 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chain2)\n",
    "\n",
    "# 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "response = chain2.invoke({\"genre\": \"Drama\"})\n",
    "print(\"\\n🔹 영화 줄거리 요약:\\n\", response) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb684fd",
   "metadata": {},
   "source": [
    "##### chain1과 chain2에서 영화 제목이 일관되게 전달 되도록 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e30883ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 영화 줄거리 요약:\n",
      "('버닝\\n'\n",
      " '\\n'\n",
      " '버닝  \\n'\n",
      " '감독: 이창동  \\n'\n",
      " '출연: 유아인, 전종서, 스티븐 연  \\n'\n",
      " '줄거리:  \\n'\n",
      " '빈곤한 청년 종수는 어릴 때 동네를 함께 돌아다녔던 희아를 우연히 만난 뒤, 그녀와 벤이라는 부유하고 신비로운 남자를 알게 된다. 어느 '\n",
      " '날 희아는 종수에게 “나는 언제든 사라질 수 있어”라고 말한 뒤 실종되고, 종수는 벤이 그녀를 해쳤을지도 모른다는 불안한 의심에 '\n",
      " '빠져든다.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추천한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 정보(제목,감독,캐스팅,줄거리)를 알려 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",  # Spring AI와 동일한 모델\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "chain2 = (\n",
    "    {\"movie\": chain1}  # chain1의 출력을 movie 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 실행: \"Drama\" 장르의 영화 추천 및 줄거리 요약\n",
    "response = chain2.invoke({\"genre\": \"Drama\"})\n",
    "print(\"\\n🔹 영화 줄거리 요약:\")\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
