{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-KIIvQ5NKJP"
   },
   "source": [
    "PDF 파일기반 질의응답 챗봇 (랭체인, 그라디오, Upstage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHgrhVbLnM6I"
   },
   "outputs": [],
   "source": [
    "# %pip install -q openai\n",
    "# %pip install -q langchain\n",
    "# %pip install -q -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5uYZQknXLDM"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSX_ndLWHJqy"
   },
   "outputs": [],
   "source": [
    "# %pip install -q pypdf\n",
    "# %pip install -q faiss-cpu\n",
    "# %pip install -q tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kZxXmSjQRHy"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VgBZ49vQVzg"
   },
   "outputs": [],
   "source": [
    "# PDF 로드\n",
    "loader = PyPDFLoader(\"../data/tutorial-korean.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VgBZ49vQVzg"
   },
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, \n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    ")\n",
    "#texts = text_splitter.split_documents(documents)\n",
    "texts = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4otdIuNkT5b_"
   },
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "# OpenAI Embeddings 적용\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "\n",
    "# FAISS 벡터 저장소 생성\n",
    "vector_store = FAISS.from_documents(texts, embeddings)\n",
    "vector_store.save_local(\"../db/faiss_db\")\n",
    "# 벡터 저장소에서 검색하기\n",
    "# search_type=\"similarity\" 옵션은 retrieval 객체에서 유사성 검색을 사용하여 질문 vector와 가장 유사한 문장 vector를 선택함\n",
    "# search_kwargs 옵션은 vector 저장소에서 Prompt 2개의 텍스트 덩어리로 보내려는 것을 의미함\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 6}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# ChatOpenAI는 기본모델인  gpt-3.5-turbo 사용하고\n",
    "# temperature=0은 보수적인 지문에 대한 답변을 내고, temperature=1 다양한 답변을 낼 수 있음 \n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 한국어 최적화 프롬프트\n",
    "prompt_template = \"\"\"\n",
    "당신은 BlueJ 프로그래밍 환경 전문가입니다. \n",
    "아래 문서 내용을 바탕으로 정확하고 친절한 답변을 제공해주세요.\n",
    "\n",
    "문서 내용:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변 규칙:\n",
    "1. 문서 내용만을 근거로 답변하세요\n",
    "2. 단계별 설명이 필요하면 순서대로 작성하세요  \n",
    "3. 구체적인 메뉴명, 버튼명을 포함하세요\n",
    "4. 문서에 없는 정보는 \"문서에서 찾을 수 없습니다\"라고 하세요\n",
    "\n",
    "답변:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "print(\" 프롬프트 설정 완료\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetrievalQA가 실질적인 RAG를 수행하는 객체\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,  # 기존 retriever 유지\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDW1hZX3a_1w"
   },
   "outputs": [],
   "source": [
    "query = \"코드패드는 무엇이고 어떻게 사용하나요?\"\n",
    "query_result = chain.invoke(query)\n",
    "print(type(query_result))\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in query_result.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(query_result['source_documents']))\n",
    "type(query_result['source_documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = query_result['source_documents'][0]\n",
    "print(type(doc))\n",
    "doc_dict = doc.model_dump()\n",
    "print(type(doc_dict))\n",
    "print(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in doc_dict:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qr0_iNsLTb6y"
   },
   "outputs": [],
   "source": [
    "doc_dict['metadata']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"애플릿을 만들고 실행하는 방법을 설명해주세요\"\n",
    "query_result = chain.invoke(query)\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "system_template = \"\"\"Use the following pieces of context to answer the user's question shortly.\n",
    "Given the following summaries of a long document and a question, create a final answer with references (\"SOURCES\"), \n",
    "use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\n",
    "\n",
    "You MUST answer in Korean and in Markdown format:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\n",
    "    \"prompt\": prompt,\n",
    "    \"document_variable_name\": \"summaries\",\n",
    "}\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # GPT-4를 사용하려면 model=\"gpt-4\"\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    input_key=\"question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVdJrxN0TzwC"
   },
   "outputs": [],
   "source": [
    "query = \"소비자 부문 AI 활용사례 무엇인가요?\"\n",
    "query_result = chain.invoke({\"question\": query})\n",
    "print(type(query_result))\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cM3DcAF4Uqvh"
   },
   "outputs": [],
   "source": [
    "print(query_result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxNF1I2pUzIv"
   },
   "outputs": [],
   "source": [
    "query_result['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSOqIBzXU6f_"
   },
   "outputs": [],
   "source": [
    "for doc in query_result['source_documents']:\n",
    "    print('내용 : ' + doc.page_content[0:100].replace('\\n', ' '))\n",
    "    print('파일 : ' + doc.metadata['source'])\n",
    "    print('페이지 : ' + str(doc.metadata['page']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_message = query_result['result']\n",
    "for i, doc in enumerate(query_result['source_documents']):\n",
    "    bot_message += '[' + str(i+1) + '] ' + doc.metadata['source'] + '(' + str(doc.metadata['page']) + ') '\n",
    "    print(bot_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHPK_8ZWHCRb"
   },
   "outputs": [],
   "source": [
    "#%pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yl_SXA37i4p5"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_upstage import UpstageEmbeddings,ChatUpstage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import gradio as gr\n",
    "\n",
    "# 1. PDF를 한 번만 로드하여 벡터 저장소 생성\n",
    "def initialize_retriever():\n",
    "\n",
    "    pdf_path = \"../data//tutorial-korean.pdf\"\n",
    "    # PDF 파일을 로드하여 문서 객체로 변환\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 문서를 일정한 크기로 분할 (chunk_size=1000, 중첩 없음)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200, \n",
    "        chunk_overlap=0,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    # 문서의 텍스트를 벡터 임베딩으로 변환\n",
    "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "    # 변환된 벡터를 FAISS 벡터 저장소에 저장\n",
    "    vector_store = FAISS.from_documents(texts, embeddings)\n",
    "    vector_store.save_local(\"../db/faiss_db\")\n",
    "    # 저장된 벡터를 검색할 수 있는 retriever 생성 (유사한 문서 2개 검색)\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\", \n",
    "        search_kwargs={\"k\": 6}\n",
    "    )\n",
    "    return retriever\n",
    "\n",
    "# 2. 전역 retriever 생성 (앱 시작 시 한 번만 실행)\n",
    "retriever = initialize_retriever()\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"코드패드는 무엇이고 어떻게 사용하나요?\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 3. 채팅 응답 함수 (retriever를 재사용)\n",
    "def chat_respond(message, chat_history):\n",
    "      #  시스템 메시지 템플릿: 문서 요약을 기반으로 질문에 답변하도록 설정\n",
    "    system_template = \"\"\"당신은 BlueJ 프로그래밍 환경 전문가입니다. \n",
    "        아래 문서 내용을 바탕으로 정확하고 친절한 답변을 제공해주세요.\n",
    "\n",
    "        문서 내용:\n",
    "        {context}\n",
    "\n",
    "        질문: {question}\n",
    "\n",
    "        답변 규칙:\n",
    "        1. 문서 내용만을 근거로 답변하세요\n",
    "        2. 단계별 설명이 필요하면 순서대로 작성하세요  \n",
    "        3. 구체적인 메뉴명, 버튼명을 포함하세요\n",
    "        4. 문서에 없는 정보는 \"문서에서 찾을 수 없습니다\"라고 하세요\n",
    "\n",
    "        답변:\n",
    "    \"\"\"\n",
    "    # 사용자 질문을 받아 최종 Prompt 구성\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    chain_type_kwargs = {\n",
    "        \"prompt\": prompt, #  LLM이 사용할 프롬프트 지정\n",
    "        \"document_variable_name\": \"context\", #  문서 요약 데이터를 LLM에 전달할 변수 이름\n",
    "    }\n",
    "    \n",
    "    llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    # RetrievalQA 체인 생성\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  #  문서를 하나의 큰 텍스트로 처리하는 방식\n",
    "        retriever=retriever, #  유사 문서를 검색하는 retriever 연결\n",
    "        return_source_documents=True, #  답변에 참조한 문서 정보 포함\n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "        input_key=\"question\" #  사용자 질문을 \"question\" 키로 전달\n",
    "    )\n",
    "    \n",
    "    # LLM을 호출하여 질문에 대한 응답 생성\n",
    "    query_result = chain.invoke({\"question\": message})\n",
    "\n",
    "    # 모델의 답변을 가져오기\n",
    "    bot_message = query_result['result']\n",
    "\n",
    "    # 참조한 문서 정보를 응답에 추가\n",
    "    for i, doc in enumerate(query_result['source_documents']):\n",
    "        bot_message += f' [{i+1}] {doc.metadata.get(\"source\", \"Unknown\")} (Page {doc.metadata.get(\"page\", \"N/A\")})'\n",
    "\n",
    "    # Gradio 채팅 기록 형식에 맞춰 응답을 저장\n",
    "    chat_history.append({\"role\": \"user\", \"content\": message})  # 사용자 메시지 추가\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": bot_message})  # 봇 응답 추가\n",
    "    \n",
    "    # 입력 창 초기화 및 갱신된 채팅 기록 반환\n",
    "    return \"\", chat_history\n",
    "\n",
    "# 4. # Gradio UI 생성 및 실행\n",
    "with gr.Blocks() as demo:  \n",
    "    # 채팅 창 (채팅 메시지를 표시하는 Gradio 컴포넌트)\n",
    "    chatbot = gr.Chatbot(label=\"채팅창\", type=\"messages\")\n",
    "    # 사용자 입력 텍스트 박스 (메시지를 입력하는 필드)\n",
    "    msg = gr.Textbox(label=\"입력\")\n",
    "    # 초기화 버튼 (채팅 기록을 초기화하는 버튼)\n",
    "    clear = gr.Button(\"초기화\")\n",
    "\n",
    "    # 사용자가 입력 필드에 메시지를 입력하면 chat_respond()가 실행됨\n",
    "    #  - 입력한 메시지는 msg에서 가져옴\n",
    "    #  - 기존 채팅 기록(chatbot)과 함께 전달됨\n",
    "    #  - 응답을 받은 후, msg를 초기화하고 chatbot에 대화 내용을 추가\n",
    "    msg.submit(chat_respond, [msg, chatbot], [msg, chatbot])  \n",
    "    # 초기화 버튼 클릭 시, 채팅 기록을 비우도록 설정\n",
    "    clear.click(lambda: [], None, chatbot, queue=False)\n",
    "\n",
    "# Gradio 앱 실행 (debug 모드 활성화)\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
